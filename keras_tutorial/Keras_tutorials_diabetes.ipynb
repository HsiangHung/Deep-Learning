{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Implementation Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# fix random seed for reproducibility\n",
    "#seed = 7\n",
    "#numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. The Data\n",
    "\n",
    "In this tutorial, we are going to study what attributes infuence Pima Indians to have diabetes. The dataset is from [UIC Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes). All samples here are females at least 21 years old of Pima Indian heritage. \n",
    "\n",
    "This is a binary classification problem, and Class = 1 means patients having diabetes. The number of instances (samples) is 768 and the number of attributes is 8, which are **(1) Number of times pregnant** \n",
    "**(2) Plasma glucose concentration a 2 hours in an oral glucose tolerance test**,\n",
    "**(3) Diastolic blood pressure (mm Hg)**,\n",
    "**(4) Triceps skin fold thickness (mm)**,\n",
    "**(5) 2-Hour serum insulin (mu U/ml)**,\n",
    "**(6) Body mass index (weight in kg/(height in m)^2)**,\n",
    "**(7) Diabetes pedigree function**,\n",
    "**(8) Age (years)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_pregnant</th>\n",
       "      <th>glucose_concentration</th>\n",
       "      <th>blood_pressure</th>\n",
       "      <th>skin_fold_thickness</th>\n",
       "      <th>serum_insulin</th>\n",
       "      <th>body_mass_index</th>\n",
       "      <th>diabetes_pedigree</th>\n",
       "      <th>age</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>116</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.6</td>\n",
       "      <td>0.201</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_pregnant  glucose_concentration  blood_pressure  skin_fold_thickness  \\\n",
       "0             1                     85              66                   29   \n",
       "1             8                    183              64                    0   \n",
       "2             1                     89              66                   23   \n",
       "3             0                    137              40                   35   \n",
       "4             5                    116              74                    0   \n",
       "\n",
       "   serum_insulin  body_mass_index  diabetes_pedigree  age  class  \n",
       "0              0             26.6              0.351   31      0  \n",
       "1              0             23.3              0.672   32      1  \n",
       "2             94             28.1              0.167   21      0  \n",
       "3            168             43.1              2.288   33      1  \n",
       "4              0             25.6              0.201   30      0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_csv(\"pima-indians-diabetes.csv\")\n",
    "dataset.columns = ['num_pregnant','glucose_concentration','blood_pressure','skin_fold_thickness','serum_insulin','body_mass_index','diabetes_pedigree','age','class']\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(767, 9)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset in training and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = dataset.iloc[:,:8]\n",
    "Y = dataset.iloc[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pandas.core.frame.DataFrame, pandas.core.series.Series)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cross_validation import cross_val_score, train_test_split\n",
    "from sklearn import metrics, cross_validation\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)\n",
    "type(X_train), type(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((536, 8), (536,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((231, 8), (231,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that different from sklearn, the **input data format in Keras needs to be np.array**. Thus here we have to convert the pandas frame to numpy array: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_arr = X_train.as_matrix()\n",
    "Y_train_arr = Y_train.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test_arr = X_test.as_matrix()\n",
    "Y_test_arr = Y_test.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, numpy.ndarray)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train_arr), type(Y_train_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define the Neural Network\n",
    "\n",
    "### (a) model I: 3-layer network, unit numbers are 12-8-1, relu activation function\n",
    "\n",
    "First we consider one-hidden layer, so the neural network architecture is **input-hidden-output**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, init='uniform', activation='relu'))\n",
    "model.add(Dense(8, init='uniform', activation='relu'))\n",
    "model.add(Dense(1, init='uniform', activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input dimension is 8, since there are 8 attributes in the data. The above neural network structure shows that the input layer has 12 neurons (units), hidden one has 8 neurons. The output always has 1 neuron. Since this is a binary classification problem, we always implement **sigmoid** activation function in the output layer to indicate probability to have diabetes. Once the probability > 0.5, the model will predict the patient of patients having diabetes, i.e. $\\hat{y}_{pred} =1$, otherwise $\\hat{y}_{pred}=0$.\n",
    "\n",
    "For input layer and hidden layers, we can still choose **sigmoid** or **[relu](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)** as the activation function. **relu** activation function is popular in deep learning, in particular for convolutional neural networks. For now we use **relu**. Later we will revisit the same network but using **sigmoid** activation and compare the performances.\n",
    "\n",
    "\n",
    "We use **logarithmic loss** as the loss (cost) function in stochastic graident descent. In Keras, for a **binary** classification problem it is **binary_crossentropy**. For multiclass classification problems, the loss function can be **categorical_crossentropy** or **mse**. To train the neural networks, we implement gradient descent to minimize the loss function. In Keras, we can choose **Adam** or **SGD** for gradient descent. **Adam** is an efficient gradient descent algorithm. Learning more about the Adam optimization algorithm, please refer to the paper [“Adam: A Method for Stochastic Optimization“](https://arxiv.org/abs/1412.6980). The metric used here is \"accuracy\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training model\n",
    "\n",
    "We first set up number of epoches as **nb_epoch=150**, and the size of each mini-batch as **batch_size=10**. Keep in mind that different from sklearn, the input data format in Keras need to be **numpy.array** (in sklearn input data can be dataframe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x114a04b70>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_arr, Y_train_arr, nb_epoch=150, batch_size=10, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352/536 [==================>...........] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  Loss:  0.510310189493  , acc: 0.753731342394\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_train_arr, Y_train_arr)\n",
    "print('  Loss: ', scores[0], ' , acc:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231/231 [==============================] - 0s     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "  Loss:  0.543567587545  , acc: 0.740259740389\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test_arr, Y_test_arr)\n",
    "print('  Loss: ', scores[0], ' , acc:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **acc** is the accuracy defined by the ratio we have right prediction. This is the same as doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.74025974026\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test_arr)\n",
    "rounded = [round(x[0]) for x in predictions]\n",
    "a = np.dot(rounded-Y_test_arr, rounded-Y_test_arr)\n",
    "print(1.0-a/len(rounded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can further compute the probability of each Pima Indian patient having diabetes as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192/231 [=======================>......] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b[[ 0.58446217]\n",
      " [ 0.40370643]\n",
      " [ 0.16911635]\n",
      " [ 0.2323039 ]\n",
      " [ 0.0529535 ]\n",
      " [ 0.20644023]\n",
      " [ 0.12511159]\n",
      " [ 0.08647307]\n",
      " [ 0.23116459]\n",
      " [ 0.42215642]]\n"
     ]
    }
   ],
   "source": [
    "print (model.predict_proba(X_test_arr)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Double mini-batch size\n",
    "\n",
    "Now we can double the mini-batch size and train the model again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32/231 [===>..........................] - ETA: 0s  Loss:  0.520820880865  , acc: 0.753246754021\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train_arr, Y_train_arr, nb_epoch=150, batch_size=20, verbose=0)\n",
    "scores = model.evaluate(X_test_arr, Y_test_arr)\n",
    "print('  Loss: ', scores[0], ' , acc:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement SGD\n",
    "\n",
    "Next we try SGD as the gradient descent method. We can see the performance is worse than that using **adam** algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "#model.compile(loss='binary_crossentropy', optimizer=SGD(lr=0.01, momentum=0.9, nesterov=True))\n",
    "model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32/231 [===>..........................] - ETA: 0s  Loss:  0.586787244974  , acc: 0.653679654454\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train_arr, Y_train_arr, nb_epoch=150, batch_size=10, verbose=0)\n",
    "scores = model.evaluate(X_test_arr, Y_test_arr);\n",
    "print('  Loss: ', scores[0], ' , acc:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) model II: 3-layer network, unit numbers are 40-30-1, relu activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Dense(40, input_dim=8, init='uniform', activation='relu'))\n",
    "model2.add(Dense(30, init='uniform', activation='relu'))\n",
    "model2.add(Dense(1, init='uniform', activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352/536 [==================>...........] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  Loss:  0.443567765738  , acc: 0.776119402985\n"
     ]
    }
   ],
   "source": [
    "model2.fit(X_train_arr, Y_train_arr, nb_epoch=150, batch_size=10, verbose=0)\n",
    "scores = model2.evaluate(X_train_arr, Y_train_arr);\n",
    "print('  Loss: ', scores[0], ' , acc:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224/231 [============================>.] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  Loss:  0.594045774761  , acc: 0.740259740389\n"
     ]
    }
   ],
   "source": [
    "scores = model2.evaluate(X_test_arr, Y_test_arr);\n",
    "print('  Loss: ', scores[0], ' , acc:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see this shows using more units will train more accurate models, but has slightly lower accuracy on test dataset. Therefore using too many neurons will result in overfitting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) model III: 3-layer network, unit numbers are 12-8-1, sigmoid activation function\n",
    "\n",
    "Now let's do deep learning by considering **sigmoid** as activation function. The accuracy is slightly better than using **relu**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32/231 [===>..........................] - ETA: 0s  Loss:  0.541187678839  , acc: 0.701298702073\n"
     ]
    }
   ],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(Dense(12, input_dim=8, init='uniform', activation='sigmoid'))\n",
    "model3.add(Dense(8, init='uniform', activation='sigmoid'))\n",
    "model3.add(Dense(1, init='uniform', activation='sigmoid'))\n",
    "model3.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model3.fit(X_train_arr, Y_train_arr, nb_epoch=150, batch_size=10, verbose=0)\n",
    "scores = model3.evaluate(X_test_arr, Y_test_arr);\n",
    "print('  Loss: ', scores[0], ' , acc:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### (d) model IV: 4-layer network, unit numbers are 12-8-8-1, relu activation function\n",
    "\n",
    "Here we use two hidden layers, the network structure is **input-hidden-hidden-output**, and numbers of units on the both hidden layers are 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32/231 [===>..........................] - ETA: 0s  Loss:  0.565378722174  , acc: 0.72294372346\n"
     ]
    }
   ],
   "source": [
    "model4 = Sequential()\n",
    "model4.add(Dense(12, input_dim=8, init='uniform', activation='relu'))\n",
    "model4.add(Dense(8, init='uniform', activation='relu'))\n",
    "model4.add(Dense(8, init='uniform', activation='relu'))\n",
    "model4.add(Dense(1, init='uniform', activation='sigmoid'))\n",
    "model4.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model4.fit(X_train_arr, Y_train_arr, nb_epoch=150, batch_size=10, verbose=0)\n",
    "scores = model4.evaluate(X_test_arr, Y_test_arr);\n",
    "print('  Loss: ', scores[0], ' , acc:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared with the previous results using three layers, the four-layer network does not significantly improve model accuracy (compared to model II)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/231 [===============>..............] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]], dtype=int32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict_classes(X_test_arr)\n",
    "predictions.shape\n",
    "predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "rounded = [round(x[0]) for x in predictions]\n",
    "print (rounded[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231/231 [==============================] - 0s     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.87228608],\n",
       "       [ 0.55333531],\n",
       "       [ 0.2439429 ],\n",
       "       [ 0.30015633],\n",
       "       [ 0.14511333],\n",
       "       [ 0.28503677],\n",
       "       [ 0.14051481],\n",
       "       [ 0.11668609],\n",
       "       [ 0.35142043],\n",
       "       [ 0.39413071]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions2 = model.predict_proba(X_test_arr)\n",
    "predictions2[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compare to Logistic-Regression, SVM and Random-Forest models\n",
    "\n",
    "Next we try to compare the neural network classification with other machine learning models, such as logistic-regression, support-vector-machine and random-forest models. We train the models with 10-fold cross-validation set,  perform grid search to find the best model and the hyperparameters, and then compute the model accuracy using the test dataset. We will see that in the current small dataset case, the logistic regression, SVM and random forest also show similar accuracy as the neural network did.\n",
    "\n",
    "### (a) Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 0.673559494314\n",
      "0.1 0.71091099676\n",
      "1 0.765182644051\n",
      "2 0.772557651992\n",
      "10 0.778219299917\n",
      "100 0.776332507465\n",
      "500 0.778219299917\n",
      "1000 0.776332507465\n",
      "5000 0.778219299917\n",
      "best reg = 500\n",
      "accuracy =  0.761904761905\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "best_logreg_model = None\n",
    "max_score = -1\n",
    "best_reg = -1\n",
    "for regularization_param in [0.01, 0.1, 1, 2, 10, 100, 500, 1000, 5000]:\n",
    "    logreg = linear_model.LogisticRegression('l2', C=regularization_param)\n",
    "    cv_score = cross_val_score(logreg, X_train, Y_train, cv=10)\n",
    "    print (regularization_param, np.mean(cv_score))\n",
    "    if np.mean(cv_score) > max_score:\n",
    "        max_score = np.mean(cv_score)\n",
    "        best_logreg_model = logreg\n",
    "        best_reg = regularization_param\n",
    "        \n",
    "best_logreg_model.fit(X_train, Y_train)\n",
    "print ('best reg =', best_reg)\n",
    "print ('accuracy = ', best_logreg_model.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Support-vector-machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 0.777933541018\n",
      "0.2 0.776064382139\n",
      "0.5 0.77978539287\n",
      "0.7 0.776047075112\n",
      "1.0 0.777916233991\n",
      "2.0 0.776047075112\n",
      "5.0 0.774177916234\n",
      "best C = 0.5\n",
      "accuracy:  0.753246753247\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "best_svm_model = None\n",
    "best_Cs = -1\n",
    "max_score = -1\n",
    "for Cs in [0.1, 0.2, 0.5, 0.7, 1.0, 2.0, 5.0]:\n",
    "    svc = svm.SVC(kernel='linear', C=Cs, probability=True)\n",
    "    ### usually not suggest to do cv in SVM since it costs time a lot, even an iteration\n",
    "    cv_score = cross_val_score(svc, X_train, Y_train, cv=5) \n",
    "    print (Cs, np.mean(cv_score))\n",
    "    if np.mean(cv_score) > max_score:\n",
    "        max_score = np.mean(cv_score)\n",
    "        best_svm_model = svc\n",
    "        best_Cs = Cs\n",
    "\n",
    "best_svm_model.fit(X_train, Y_train)\n",
    "print ('best C =', best_Cs)\n",
    "print ('accuracy = ', best_svm_model.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE =  0.753246753247\n"
     ]
    }
   ],
   "source": [
    "print ('MSE = ', best_svm_model.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hhhung/anaconda/envs/tensorflow/lib/python3.5/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import grid_search\n",
    "rf = RandomForestClassifier()\n",
    "parameters = {'n_estimators': [4,6,8],'max_depth':[5,10,15],'min_samples_leaf':[10,20]}\n",
    "model_cv_grid = grid_search.GridSearchCV(rf, parameters, scoring='roc_auc', verbose=0, n_jobs=-1, cv=10)\n",
    "model_cv_grid.fit(X_train,Y_train)\n",
    "best_rf_model = model_cv_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=5, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=10,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=6, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_rf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy =  0.744588744589\n"
     ]
    }
   ],
   "source": [
    "print ('accuracy = ', best_rf_model.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "      <th>variable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.323314</td>\n",
       "      <td>glucose_concentration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.190571</td>\n",
       "      <td>body_mass_index</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.125323</td>\n",
       "      <td>diabetes_pedigree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.115889</td>\n",
       "      <td>age</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.094110</td>\n",
       "      <td>serum_insulin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   importance               variable\n",
       "0    0.323314  glucose_concentration\n",
       "1    0.190571        body_mass_index\n",
       "2    0.125323      diabetes_pedigree\n",
       "3    0.115889                    age\n",
       "4    0.094110          serum_insulin"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance = best_rf_model.feature_importances_\n",
    "attribute = X.columns\n",
    "\n",
    "v = sorted(range(len(importance)), key=lambda k: importance[k], reverse=True)\n",
    "sorted_importance = [importance[i] for i in v]\n",
    "sorted_attribute = [attribute[i] for i in v]\n",
    "\n",
    "df_importance = pd.DataFrame({'variable': sorted_attribute, 'importance' : sorted_importance})\n",
    "df_importance.sort_index().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "* 1. [Develop Your First Neural Network in Python With Keras Step-By-Step](http://machinelearningmastery.com/tutorial-first-neural-network-python-keras/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
