{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Implementation Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. The Data\n",
    "\n",
    "In this tutorial, we are going to study what attributes infuence Pima Indians to have diabetes. The dataset is from [UIC Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes). All samples here are females at least 21 years old of Pima Indian heritage. \n",
    "\n",
    "This is a binary classification problem, and Class = 1 means patients having diabetes. The number of instances (samples) is 768 and the number of attributes is 8, which are **(1) Number of times pregnant** \n",
    "**(2) Plasma glucose concentration a 2 hours in an oral glucose tolerance test**,\n",
    "**(3) Diastolic blood pressure (mm Hg)**,\n",
    "**(4) Triceps skin fold thickness (mm)**,\n",
    "**(5) 2-Hour serum insulin (mu U/ml)**,\n",
    "**(6) Body mass index (weight in kg/(height in m)^2)**,\n",
    "**(7) Diabetes pedigree function**,\n",
    "**(8) Age (years)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_pregnant</th>\n",
       "      <th>glucose_concentration</th>\n",
       "      <th>blood_pressure</th>\n",
       "      <th>skin_fold_thickness</th>\n",
       "      <th>serum_insulin</th>\n",
       "      <th>body_mass_index</th>\n",
       "      <th>diabetes_pedigree</th>\n",
       "      <th>age</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>116</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.6</td>\n",
       "      <td>0.201</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_pregnant  glucose_concentration  blood_pressure  skin_fold_thickness  \\\n",
       "0             1                     85              66                   29   \n",
       "1             8                    183              64                    0   \n",
       "2             1                     89              66                   23   \n",
       "3             0                    137              40                   35   \n",
       "4             5                    116              74                    0   \n",
       "\n",
       "   serum_insulin  body_mass_index  diabetes_pedigree  age  class  \n",
       "0              0             26.6              0.351   31      0  \n",
       "1              0             23.3              0.672   32      1  \n",
       "2             94             28.1              0.167   21      0  \n",
       "3            168             43.1              2.288   33      1  \n",
       "4              0             25.6              0.201   30      0  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_csv(\"pima-indians-diabetes.csv\")\n",
    "dataset.columns = ['num_pregnant','glucose_concentration','blood_pressure','skin_fold_thickness','serum_insulin','body_mass_index','diabetes_pedigree','age','class']\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(767, 9)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset in training and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = dataset.iloc[:,:8]\n",
    "Y = dataset.iloc[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pandas.core.frame.DataFrame, pandas.core.series.Series)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cross_validation import cross_val_score, train_test_split\n",
    "from sklearn import metrics, cross_validation\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)\n",
    "type(X_train), type(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((536, 8), (536,))"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((231, 8), (231,))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that different from Sklearn, the input data format in Keras need to be array. Thus here we have to convert the pandas frame to numpy array: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_arr = X_train.as_matrix()\n",
    "Y_train_arr = Y_train.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test_arr = X_test.as_matrix()\n",
    "Y_test_arr = Y_test.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, numpy.ndarray)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train_arr), type(Y_train_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define the Neural Network\n",
    "\n",
    "### (a) model I: 3-layer network, neuron numbers are 12-8-1, relu activation function\n",
    "\n",
    "First we consider one-hidden layer, so the neural network architecure is **input-hidden-output**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, init='uniform', activation='relu'))\n",
    "model.add(Dense(8, init='uniform', activation='relu'))\n",
    "model.add(Dense(1, init='uniform', activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input layer has 12 neurons, hidden one has 8 neurons and the output always has 1 neuron. Since this is binary classification problem, we always implement **sigmoid** activation function in the output layerto indicate probability to have diabetes.\n",
    "\n",
    "For input layer and hidden layers, we can still choose **sigmoid** or even use **[relu](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)** as the activation function. **relu** activation function is popular in deep learning, in particular in convolutional neural networks. Later we will revisit the same problem but using **sigmoid** activation and compare the performances.\n",
    "\n",
    "\n",
    "We use **logarithmic loss**, which for a **binary** classification problem is defined in Keras as **binary_crossentropy**, as the metric for stochastic graident descent. In Keras, **Adam** is an efficient gradient descent algorithm. Learn more about the Adam optimization algorithm in the paper [“Adam: A Method for Stochastic Optimization“](https://arxiv.org/abs/1412.6980). The metric used here is \"accuracy\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training model\n",
    "\n",
    "We first set up number of epoches as **nb_epoch=150**, and the size of each mini-batch as **batch_size=10**. Keep in mind that different from sklean, the input data format in Keras should be array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x115e07a58>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_arr, Y_train_arr, nb_epoch=150, batch_size=10, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224/536 [===========>..................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  Loss:  0.369905427765  , acc: 0.819029849857\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_train_arr, Y_train_arr)\n",
    "print('  Loss: ', scores[0], ' , acc:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231/231 [==============================] - 0s     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "  Loss:  0.577939843074  , acc: 0.757575757834\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test_arr, Y_test_arr)\n",
    "print('  Loss: ', scores[0], ' , acc:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **acc** is the accuracy defined the ratio we have right prediction. This is the same as doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.757575757576\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test_arr)\n",
    "rounded = [round(x[0]) for x in predictions]\n",
    "a = numpy.dot(rounded-Y_test_arr, rounded-Y_test_arr)\n",
    "print(1.0-a/len(rounded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can further compute the probability of each Pima Indian patient having diabetes as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224/231 [============================>.] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b[[ 0.18161899]\n",
      " [ 0.2329852 ]\n",
      " [ 0.55269057]\n",
      " [ 0.14207026]\n",
      " [ 0.15011092]\n",
      " [ 0.10766293]\n",
      " [ 0.85442162]\n",
      " [ 0.02469895]\n",
      " [ 0.21705227]\n",
      " [ 0.08255263]]\n"
     ]
    }
   ],
   "source": [
    "print (model.predict_proba(X_test_arr)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Double mini-batch size\n",
    "\n",
    "Now we can double the mini-batch size and train the model again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32/231 [===>..........................] - ETA: 0s  Loss:  0.675078886928  , acc: 0.696969697228\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train_arr, Y_train_arr, nb_epoch=150, batch_size=20, verbose=0)\n",
    "scores = model.evaluate(X_test_arr, Y_test_arr)\n",
    "print('  Loss: ', scores[0], ' , acc:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement SGD\n",
    "\n",
    "Next we try SGD as the gradient descent method. We can see the performance is worse than that using **adam** algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "#model.compile(loss='binary_crossentropy', optimizer=SGD(lr=0.01, momentum=0.9, nesterov=True))\n",
    "model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32/231 [===>..........................] - ETA: 0s  Loss:  0.647219739693  , acc: 0.649350650125\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train_arr, Y_train_arr, nb_epoch=150, batch_size=10, verbose=0)\n",
    "scores = model.evaluate(X_test_arr, Y_test_arr);\n",
    "print('  Loss: ', scores[0], ' , acc:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) model II: 3-layer network, neuron numbers are 40-30-1, relu activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Dense(40, input_dim=8, init='uniform', activation='relu'))\n",
    "model2.add(Dense(30, init='uniform', activation='relu'))\n",
    "model2.add(Dense(1, init='uniform', activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/536 [===========================>..] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  Loss:  0.202747819139  , acc: 0.902985074627\n"
     ]
    }
   ],
   "source": [
    "model2.fit(X_train_arr, Y_train_arr, nb_epoch=150, batch_size=10, verbose=0)\n",
    "scores = model2.evaluate(X_train_arr, Y_train_arr);\n",
    "print('  Loss: ', scores[0], ' , acc:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192/231 [=======================>......] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  Loss:  1.3751760489  , acc: 0.692640693157\n"
     ]
    }
   ],
   "source": [
    "scores = model2.evaluate(X_test_arr, Y_test_arr);\n",
    "print('  Loss: ', scores[0], ' , acc:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see this shows using more nodes will train more accurate models, but has lower accuracy on test dataset. Therefore using too many neurons will result in overfitting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) model III: 3-layer network, neuron numbers are 12-8-1, sigmoid activation function\n",
    "\n",
    "Now let's do deep learning by considering **sigmoid** as activation function. The accuracy is worse than using **relu**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32/231 [===>..........................] - ETA: 0s  Loss:  0.555261078851  , acc: 0.72294372346\n"
     ]
    }
   ],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(Dense(12, input_dim=8, init='uniform', activation='sigmoid'))\n",
    "model3.add(Dense(8, init='uniform', activation='sigmoid'))\n",
    "model3.add(Dense(1, init='uniform', activation='sigmoid'))\n",
    "model3.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model3.fit(X_train_arr, Y_train_arr, nb_epoch=150, batch_size=10, verbose=0)\n",
    "scores = model3.evaluate(X_test_arr, Y_test_arr);\n",
    "print('  Loss: ', scores[0], ' , acc:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### (d) model IV: 4-layer network, neuron numbers are 12-8-8-1, relu activation function\n",
    "\n",
    "Here we use two hidden layers and numbers of nodes on the both layers are 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32/231 [===>..........................] - ETA: 0s  Loss:  0.574795645037  , acc: 0.748917749176\n"
     ]
    }
   ],
   "source": [
    "model4 = Sequential()\n",
    "model4.add(Dense(12, input_dim=8, init='uniform', activation='relu'))\n",
    "model4.add(Dense(8, init='uniform', activation='relu'))\n",
    "model4.add(Dense(8, init='uniform', activation='relu'))\n",
    "model4.add(Dense(1, init='uniform', activation='sigmoid'))\n",
    "model4.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model4.fit(X_train_arr, Y_train_arr, nb_epoch=150, batch_size=10, verbose=0)\n",
    "scores = model4.evaluate(X_test_arr, Y_test_arr);\n",
    "print('  Loss: ', scores[0], ' , acc:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared with the previous results using three layers, the four-layer network does not improve model accuracy. This indicates that in our current case using more hidden could be not necessary (this is because our dataset is small)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.02870761]\n",
      " [ 0.2803773 ]\n",
      " [ 0.33776325]\n",
      " [ 0.64599019]\n",
      " [ 0.36038035]\n",
      " [ 0.14201277]\n",
      " [ 0.08908633]\n",
      " [ 0.12896512]\n",
      " [ 0.85859615]\n",
      " [ 0.89892173]]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test)\n",
    "predictions.shape\n",
    "print (predictions[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "rounded = [round(x[0]) for x in predictions]\n",
    "print (rounded[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192/231 [=======================>......] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    }
   ],
   "source": [
    "predictions2 = model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compare to Logistic-Regression, SVM and Random-Forest models\n",
    "\n",
    "Next we try to compare the neural network classification with other machine learning models, such as logistic-regression, support-vector-machine and random-forest models. We train the models with 10-fold cross-validation set,  perform grid search to find the best model and the hyperparameters, and then compute the model accuracy using the test dataset. We will see that in the current small dataset case, the logistic regression, SVM and random forest also show similar accuracy as the neural network did.\n",
    "\n",
    "### (a) Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 0.686617749825\n",
      "0.1 0.720230607966\n",
      "1 0.781656184486\n",
      "2 0.787281621244\n",
      "10 0.783647798742\n",
      "100 0.783682739343\n",
      "500 0.783682739343\n",
      "1000 0.783682739343\n",
      "5000 0.783682739343\n",
      "best reg = 2\n",
      "accuracy =  0.761904761905\n"
     ]
    }
   ],
   "source": [
    "best_logreg_model = None\n",
    "max_score = -1\n",
    "best_reg = -1\n",
    "for regularization_param in [0.01, 0.1, 1, 2, 10, 100, 500, 1000, 5000]:\n",
    "    logreg = linear_model.LogisticRegression('l2', C=regularization_param)\n",
    "    cv_score = cross_val_score(logreg, X_train, Y_train, cv=10)\n",
    "    print (regularization_param, np.mean(cv_score))\n",
    "    if np.mean(cv_score) > max_score:\n",
    "        max_score = np.mean(cv_score)\n",
    "        best_logreg_model = logreg\n",
    "        best_reg = regularization_param\n",
    "        \n",
    "best_logreg_model.fit(X_train, Y_train)\n",
    "print ('best reg =', best_reg)\n",
    "print ('accuracy = ', best_logreg_model.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Support-vector-machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 0.777933541018\n",
      "0.2 0.776064382139\n",
      "0.5 0.77978539287\n",
      "0.7 0.776047075112\n",
      "1.0 0.777916233991\n",
      "2.0 0.776047075112\n",
      "5.0 0.774177916234\n",
      "best C = 0.5\n",
      "accuracy:  0.753246753247\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "best_svm_model = None\n",
    "best_Cs = -1\n",
    "max_score = -1\n",
    "for Cs in [0.1, 0.2, 0.5, 0.7, 1.0, 2.0, 5.0]:\n",
    "    svc = svm.SVC(kernel='linear', C=Cs, probability=True)\n",
    "    ### usually not suggest to do cv in SVM since it costs time a lot, even an iteration\n",
    "    cv_score = cross_val_score(svc, X_train, Y_train, cv=5) \n",
    "    print (Cs, np.mean(cv_score))\n",
    "    if np.mean(cv_score) > max_score:\n",
    "        max_score = np.mean(cv_score)\n",
    "        best_svm_model = svc\n",
    "        best_Cs = Cs\n",
    "\n",
    "best_svm_model.fit(X_train, Y_train)\n",
    "print ('best C =', best_Cs)\n",
    "print ('accuracy = ', best_svm_model.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE =  0.753246753247\n"
     ]
    }
   ],
   "source": [
    "print ('MSE = ', best_svm_model.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n",
      "[CV] max_depth=5, min_samples_leaf=10, n_estimators=4 ................\n",
      "[CV] max_depth=5, min_samples_leaf=10, n_estimators=4 ................\n",
      "[CV] max_depth=5, min_samples_leaf=10, n_estimators=4 ................\n",
      "[CV] max_depth=5, min_samples_leaf=10, n_estimators=6 ................\n",
      "[CV] ....... max_depth=5, min_samples_leaf=10, n_estimators=4 -   0.0s\n",
      "[CV] ....... max_depth=5, min_samples_leaf=10, n_estimators=4 -   0.0s\n",
      "[CV] ....... max_depth=5, min_samples_leaf=10, n_estimators=4 -   0.0s\n",
      "[CV] max_depth=5, min_samples_leaf=10, n_estimators=6 ................\n",
      "[CV] max_depth=5, min_samples_leaf=10, n_estimators=6 ................\n",
      "[CV] ....... max_depth=5, min_samples_leaf=10, n_estimators=6 -   0.0s\n",
      "[CV] max_depth=5, min_samples_leaf=10, n_estimators=8 ................\n",
      "[CV] max_depth=5, min_samples_leaf=10, n_estimators=8 ................\n",
      "[CV] ....... max_depth=5, min_samples_leaf=10, n_estimators=6 -   0.1s\n",
      "[CV] ....... max_depth=5, min_samples_leaf=10, n_estimators=8 -   0.1s\n",
      "[CV] max_depth=5, min_samples_leaf=20, n_estimators=4 ................\n",
      "[CV] max_depth=5, min_samples_leaf=10, n_estimators=8 ................\n",
      "[CV] ....... max_depth=5, min_samples_leaf=10, n_estimators=8 -   0.1s\n",
      "[CV] max_depth=5, min_samples_leaf=20, n_estimators=6 ................\n",
      "[CV] ....... max_depth=5, min_samples_leaf=10, n_estimators=6 -   0.1s\n",
      "[CV] ....... max_depth=5, min_samples_leaf=20, n_estimators=4 -   0.0s\n",
      "[CV] max_depth=5, min_samples_leaf=20, n_estimators=6 ................\n",
      "[CV] ....... max_depth=5, min_samples_leaf=10, n_estimators=8 -   0.0s\n",
      "[CV] max_depth=5, min_samples_leaf=20, n_estimators=4 ................\n",
      "[CV] max_depth=5, min_samples_leaf=20, n_estimators=4 ................\n",
      "[CV] ....... max_depth=5, min_samples_leaf=20, n_estimators=6 -   0.0s\n",
      "[CV] max_depth=5, min_samples_leaf=20, n_estimators=6 ................\n",
      "[CV] ....... max_depth=5, min_samples_leaf=20, n_estimators=4 -   0.0s\n",
      "[CV] ....... max_depth=5, min_samples_leaf=20, n_estimators=6 -   0.0s\n",
      "[CV] max_depth=5, min_samples_leaf=20, n_estimators=8 ................\n",
      "[CV] max_depth=5, min_samples_leaf=20, n_estimators=8 ................\n",
      "[CV] ....... max_depth=5, min_samples_leaf=20, n_estimators=4 -   0.0s\n",
      "[CV] ....... max_depth=5, min_samples_leaf=20, n_estimators=6 -   0.0s\n",
      "[CV] max_depth=10, min_samples_leaf=10, n_estimators=4 ...............\n",
      "[CV] ....... max_depth=5, min_samples_leaf=20, n_estimators=8 -   0.0s\n",
      "[CV] max_depth=10, min_samples_leaf=10, n_estimators=4 ...............\n",
      "[CV] ....... max_depth=5, min_samples_leaf=20, n_estimators=8 -   0.0s\n",
      "[CV] max_depth=10, min_samples_leaf=10, n_estimators=6 ...............\n",
      "[CV] max_depth=5, min_samples_leaf=20, n_estimators=8 ................\n",
      "[CV] ...... max_depth=10, min_samples_leaf=10, n_estimators=4 -   0.0s\n",
      "[CV] ...... max_depth=10, min_samples_leaf=10, n_estimators=4 -   0.0s\n",
      "[CV] max_depth=10, min_samples_leaf=10, n_estimators=4 ...............\n",
      "[CV] max_depth=10, min_samples_leaf=10, n_estimators=6 ...............\n",
      "[CV] ...... max_depth=10, min_samples_leaf=10, n_estimators=4 -   0.0s\n",
      "[CV] ...... max_depth=10, min_samples_leaf=10, n_estimators=6 -   0.0s\n",
      "[CV] max_depth=10, min_samples_leaf=10, n_estimators=8 ...............\n",
      "[CV] max_depth=10, min_samples_leaf=10, n_estimators=6 ...............\n",
      "[CV] ....... max_depth=5, min_samples_leaf=20, n_estimators=8 -   0.0s\n",
      "[CV] max_depth=10, min_samples_leaf=10, n_estimators=8 ...............\n",
      "[CV] ...... max_depth=10, min_samples_leaf=10, n_estimators=6 -   0.0s\n",
      "[CV] max_depth=10, min_samples_leaf=20, n_estimators=4 ...............\n",
      "[CV] ...... max_depth=10, min_samples_leaf=10, n_estimators=6 -   0.0s\n",
      "[CV] ...... max_depth=10, min_samples_leaf=10, n_estimators=8 -   0.0s\n",
      "[CV] max_depth=10, min_samples_leaf=20, n_estimators=6 ...............\n",
      "[CV] ...... max_depth=10, min_samples_leaf=20, n_estimators=4 -   0.0s\n",
      "[CV] max_depth=10, min_samples_leaf=20, n_estimators=4 ...............\n",
      "[CV] max_depth=10, min_samples_leaf=10, n_estimators=8 ...............\n",
      "[CV] ...... max_depth=10, min_samples_leaf=10, n_estimators=8 -   0.1s\n",
      "[CV] max_depth=10, min_samples_leaf=20, n_estimators=4 ...............\n",
      "[CV] ...... max_depth=10, min_samples_leaf=20, n_estimators=6 -   0.0s\n",
      "[CV] ...... max_depth=10, min_samples_leaf=20, n_estimators=4 -   0.0s\n",
      "[CV] ...... max_depth=10, min_samples_leaf=10, n_estimators=8 -   0.0s\n",
      "[CV] max_depth=10, min_samples_leaf=20, n_estimators=6 ...............\n",
      "[CV] max_depth=10, min_samples_leaf=20, n_estimators=8 ...............\n",
      "[CV] max_depth=10, min_samples_leaf=20, n_estimators=6 ...............\n",
      "[CV] ...... max_depth=10, min_samples_leaf=20, n_estimators=4 -   0.0s\n",
      "[CV] max_depth=15, min_samples_leaf=10, n_estimators=4 ...............\n",
      "[CV] ...... max_depth=10, min_samples_leaf=20, n_estimators=6 -   0.0s\n",
      "[CV] ...... max_depth=10, min_samples_leaf=20, n_estimators=6 -   0.0s\n",
      "[CV] max_depth=10, min_samples_leaf=20, n_estimators=8 ...............\n",
      "[CV] ...... max_depth=10, min_samples_leaf=20, n_estimators=8 -   0.0s\n",
      "[CV] max_depth=10, min_samples_leaf=20, n_estimators=8 ...............\n",
      "[CV] max_depth=15, min_samples_leaf=10, n_estimators=4 ...............\n",
      "[CV] ...... max_depth=15, min_samples_leaf=10, n_estimators=4 -   0.0s\n",
      "[CV] max_depth=15, min_samples_leaf=10, n_estimators=4 ...............\n",
      "[CV] ...... max_depth=15, min_samples_leaf=10, n_estimators=4 -   0.0s\n",
      "[CV] max_depth=15, min_samples_leaf=10, n_estimators=6 ...............\n",
      "[CV] ...... max_depth=10, min_samples_leaf=20, n_estimators=8 -   0.0s\n",
      "[CV] ...... max_depth=10, min_samples_leaf=20, n_estimators=8 -   0.0s\n",
      "[CV] max_depth=15, min_samples_leaf=10, n_estimators=6 ...............\n",
      "[CV] max_depth=15, min_samples_leaf=10, n_estimators=8 ...............\n",
      "[CV] ...... max_depth=15, min_samples_leaf=10, n_estimators=4 -   0.0s\n",
      "[CV] max_depth=15, min_samples_leaf=10, n_estimators=8 ...............\n",
      "[CV] ...... max_depth=15, min_samples_leaf=10, n_estimators=6 -   0.0s\n",
      "[CV] ...... max_depth=15, min_samples_leaf=10, n_estimators=6 -   0.0s\n",
      "[CV] ...... max_depth=15, min_samples_leaf=10, n_estimators=8 -   0.0s\n",
      "[CV] max_depth=15, min_samples_leaf=20, n_estimators=4 ...............\n",
      "[CV] max_depth=15, min_samples_leaf=10, n_estimators=8 ...............\n",
      "[CV] max_depth=15, min_samples_leaf=10, n_estimators=6 ...............\n",
      "[CV] ...... max_depth=15, min_samples_leaf=10, n_estimators=8 -   0.0s\n",
      "[CV] ...... max_depth=15, min_samples_leaf=20, n_estimators=4 -   0.0s\n",
      "[CV] max_depth=15, min_samples_leaf=20, n_estimators=4 ...............\n",
      "[CV] max_depth=15, min_samples_leaf=20, n_estimators=4 ...............\n",
      "[CV] ...... max_depth=15, min_samples_leaf=10, n_estimators=6 -   0.0s\n",
      "[CV] max_depth=15, min_samples_leaf=20, n_estimators=6 ...............\n",
      "[CV] ...... max_depth=15, min_samples_leaf=20, n_estimators=4 -   0.0s\n",
      "[CV] ...... max_depth=15, min_samples_leaf=10, n_estimators=8 -   0.0s\n",
      "[CV] ...... max_depth=15, min_samples_leaf=20, n_estimators=4 -   0.0s\n",
      "[CV] max_depth=15, min_samples_leaf=20, n_estimators=8 ...............\n",
      "[CV] max_depth=15, min_samples_leaf=20, n_estimators=6 ...............\n",
      "[CV] ...... max_depth=15, min_samples_leaf=20, n_estimators=6 -   0.0s\n",
      "[CV] max_depth=15, min_samples_leaf=20, n_estimators=6 ...............\n",
      "[CV] ...... max_depth=15, min_samples_leaf=20, n_estimators=8 -   0.0s\n",
      "[CV] ...... max_depth=15, min_samples_leaf=20, n_estimators=6 -   0.0s\n",
      "[CV] max_depth=15, min_samples_leaf=20, n_estimators=8 ...............\n",
      "[CV] max_depth=15, min_samples_leaf=20, n_estimators=8 ...............\n",
      "[CV] ...... max_depth=15, min_samples_leaf=20, n_estimators=6 -   0.0s\n",
      "[CV] ...... max_depth=15, min_samples_leaf=20, n_estimators=8 -   0.0s\n",
      "[CV] ...... max_depth=15, min_samples_leaf=20, n_estimators=8 -   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  54 out of  54 | elapsed:    0.9s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import grid_search\n",
    "rf = RandomForestClassifier()\n",
    "parameters = {'n_estimators': [4,6,8],'max_depth':[5,10,15],'min_samples_leaf':[10,20]}\n",
    "model_cv_grid = grid_search.GridSearchCV(rf,parameters,scoring='roc_auc',verbose=2,n_jobs=-1)\n",
    "model_cv_grid.fit(X_train,Y_train)\n",
    "best_rf_model = model_cv_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=5, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=20,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=8, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_rf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy =  0.757575757576\n"
     ]
    }
   ],
   "source": [
    "print ('accuracy = ', best_rf_model.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "      <th>variable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.339148</td>\n",
       "      <td>glucose_concentration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.217821</td>\n",
       "      <td>body_mass_index</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.126481</td>\n",
       "      <td>age</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.107890</td>\n",
       "      <td>diabetes_pedigree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.072164</td>\n",
       "      <td>num_pregnant</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   importance               variable\n",
       "0    0.339148  glucose_concentration\n",
       "1    0.217821        body_mass_index\n",
       "2    0.126481                    age\n",
       "3    0.107890      diabetes_pedigree\n",
       "4    0.072164           num_pregnant"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance = best_rf_model.feature_importances_\n",
    "attribute = X.columns\n",
    "\n",
    "v = sorted(range(len(importance)), key=lambda k: importance[k], reverse=True)\n",
    "sorted_importance = [importance[i] for i in v]\n",
    "sorted_attribute = [attribute[i] for i in v]\n",
    "\n",
    "df_importance = pd.DataFrame({'variable': sorted_attribute, 'importance' : sorted_importance})\n",
    "df_importance.sort_index().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "* 1. [Develop Your First Neural Network in Python With Keras Step-By-Step](http://machinelearningmastery.com/tutorial-first-neural-network-python-keras/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
