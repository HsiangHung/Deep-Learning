{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Dataset - Recongition of Handwritten Digits\n",
    "\n",
    "In this note, we will implement Keras to build neural network models to recognize handwritten digits using **mnist** data. Here my backend is tensorflow. We will show that by properly choosing the network structures and parameters, the best model trained can have accuracy close to 100% and 98% on the test dataset. Through the experiments, the best neural network shown is to consider a three-hidden-layer network using **relu** activaction function. The model structure is also sensitive to initialization and choice of loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. The MNIST database\n",
    "\n",
    "The handwritten digits data can be obtained from [Yann LeCun's website](http://yann.lecun.com/exdb/mnist/). Each image is 28 pixels by 28 pixels and is labeled 0-9 to identify the digits 0-9. This is a 10-class classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.contrib.learn.python.learn.datasets.base.Datasets"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are 55000 training datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((55000, 784), (55000, 10))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images.shape, mnist.train.labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### and there are 10000 test datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 784), (10000, 10))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.test.images.shape, mnist.test.labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the MNIST images are represented in a784-dimensional vector space with real number [0,1]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.38039219,  0.37647063,  0.3019608 ,\n",
       "        0.46274513,  0.2392157 ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.35294119,  0.5411765 ,  0.92156869,\n",
       "        0.92156869,  0.92156869,  0.92156869,  0.92156869,  0.92156869,\n",
       "        0.98431379,  0.98431379,  0.97254908,  0.99607849,  0.96078438,\n",
       "        0.92156869,  0.74509805,  0.08235294,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.54901963,\n",
       "        0.98431379,  0.99607849,  0.99607849,  0.99607849,  0.99607849,\n",
       "        0.99607849,  0.99607849,  0.99607849,  0.99607849,  0.99607849,\n",
       "        0.99607849,  0.99607849,  0.99607849,  0.99607849,  0.99607849,\n",
       "        0.74117649,  0.09019608,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.88627458,  0.99607849,  0.81568635,\n",
       "        0.78039223,  0.78039223,  0.78039223,  0.78039223,  0.54509807,\n",
       "        0.2392157 ,  0.2392157 ,  0.2392157 ,  0.2392157 ,  0.2392157 ,\n",
       "        0.50196081,  0.8705883 ,  0.99607849,  0.99607849,  0.74117649,\n",
       "        0.08235294,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.14901961,  0.32156864,  0.0509804 ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.13333334,\n",
       "        0.83529419,  0.99607849,  0.99607849,  0.45098042,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.32941177,  0.99607849,\n",
       "        0.99607849,  0.91764712,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.32941177,  0.99607849,  0.99607849,  0.91764712,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.41568631,  0.6156863 ,\n",
       "        0.99607849,  0.99607849,  0.95294124,  0.20000002,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.09803922,  0.45882356,  0.89411771,  0.89411771,\n",
       "        0.89411771,  0.99215692,  0.99607849,  0.99607849,  0.99607849,\n",
       "        0.99607849,  0.94117653,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.26666668,  0.4666667 ,  0.86274517,\n",
       "        0.99607849,  0.99607849,  0.99607849,  0.99607849,  0.99607849,\n",
       "        0.99607849,  0.99607849,  0.99607849,  0.99607849,  0.55686277,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.14509805,  0.73333335,\n",
       "        0.99215692,  0.99607849,  0.99607849,  0.99607849,  0.87450987,\n",
       "        0.80784321,  0.80784321,  0.29411766,  0.26666668,  0.84313732,\n",
       "        0.99607849,  0.99607849,  0.45882356,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.44313729,  0.8588236 ,  0.99607849,  0.94901967,  0.89019614,\n",
       "        0.45098042,  0.34901962,  0.12156864,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.7843138 ,  0.99607849,  0.9450981 ,\n",
       "        0.16078432,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.66274512,  0.99607849,\n",
       "        0.6901961 ,  0.24313727,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.18823531,\n",
       "        0.90588242,  0.99607849,  0.91764712,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.07058824,  0.48627454,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.32941177,  0.99607849,  0.99607849,\n",
       "        0.65098041,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.54509807,  0.99607849,  0.9333334 ,  0.22352943,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.82352948,  0.98039222,  0.99607849,\n",
       "        0.65882355,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.94901967,  0.99607849,  0.93725497,  0.22352943,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.34901962,  0.98431379,  0.9450981 ,\n",
       "        0.33725491,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.01960784,\n",
       "        0.80784321,  0.96470594,  0.6156863 ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.01568628,  0.45882356,  0.27058825,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, tuple)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mnist.train.images), type(mnist.train.labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotExamples(data, labels):\n",
    "    plt.figure(figsize=(8,5))\n",
    "    for i in range(8):\n",
    "        sub = 241 + i\n",
    "        ax = plt.subplot(sub)\n",
    "        index = np.random.randint(0, data.shape[0])\n",
    "        ax.set_title(\"num: \" + str(np.argmax(labels[index])))\n",
    "        im = np.reshape(data[index], (28, 28))\n",
    "        plt.imshow(im, cmap=\"gray\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeMAAAEkCAYAAAAcmlk0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114696860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotExamples(mnist.train.images, mnist.train.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Training Neural Networks using Keras\n",
    "\n",
    "Here we consider different neural network structures to train models. We compare the models by different number of neurons in hidden layers, different loss functions: **mse** and **categorical_crossentropy**, different activation functions: **sigoid** and **relu**, more hidden layers and even initialization. The neural network in the **mnist** data always has the input layer with 784 inputs and the output layer with 10 neurons (10 classes). \n",
    "\n",
    "Let us start with the neural network: one hidden layer with 100 neurons, using **sigmoid** as activation function in the hidden layer and output layer, and uniform initialization. For each variant, we will reveal how the structure infleunce the training results by comparing model performance. \n",
    "\n",
    "### (a) Initial network: 1-hidden layer with 100 neurons, sigmoid activation in all layers\n",
    "\n",
    "At the beginning we compare using **mse** and **categorical_crossentropy** as loss functions. The result shows using **categorical_crossentropy** as loss functions has faster convergence, so we will stick on using **categorical_crossentropy**. Later we will revisit using **mse**.\n",
    "\n",
    "#### Mean squared error (mse) as the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100, input_dim=784, init='uniform', activation='sigmoid'))\n",
    "model.add(Dense(10, init='uniform', activation='sigmoid'))\n",
    "## lr is the learning rate\n",
    "sgd = SGD(lr=0.1) \n",
    "model.compile(loss='mse', optimizer=sgd, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "4s - loss: 0.0914 - acc: 0.1436\n",
      "Epoch 2/20\n",
      "4s - loss: 0.0886 - acc: 0.2429\n",
      "Epoch 3/20\n",
      "4s - loss: 0.0862 - acc: 0.2760\n",
      "Epoch 4/20\n",
      "4s - loss: 0.0802 - acc: 0.3622\n",
      "Epoch 5/20\n",
      "4s - loss: 0.0714 - acc: 0.5279\n",
      "Epoch 6/20\n",
      "4s - loss: 0.0624 - acc: 0.6250\n",
      "Epoch 7/20\n",
      "4s - loss: 0.0545 - acc: 0.6898\n",
      "Epoch 8/20\n",
      "5s - loss: 0.0484 - acc: 0.7401\n",
      "Epoch 9/20\n",
      "5s - loss: 0.0437 - acc: 0.7828\n",
      "Epoch 10/20\n",
      "4s - loss: 0.0399 - acc: 0.8101\n",
      "Epoch 11/20\n",
      "4s - loss: 0.0368 - acc: 0.8306\n",
      "Epoch 12/20\n",
      "4s - loss: 0.0342 - acc: 0.8440\n",
      "Epoch 13/20\n",
      "4s - loss: 0.0321 - acc: 0.8543\n",
      "Epoch 14/20\n",
      "4s - loss: 0.0303 - acc: 0.8613\n",
      "Epoch 15/20\n",
      "4s - loss: 0.0287 - acc: 0.8678\n",
      "Epoch 16/20\n",
      "4s - loss: 0.0274 - acc: 0.8725\n",
      "Epoch 17/20\n",
      "4s - loss: 0.0263 - acc: 0.8760\n",
      "Epoch 18/20\n",
      "4s - loss: 0.0253 - acc: 0.8786\n",
      "Epoch 19/20\n",
      "4s - loss: 0.0245 - acc: 0.8816\n",
      "Epoch 20/20\n",
      "4s - loss: 0.0237 - acc: 0.8834\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1245cc6d8>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(mnist.train.images, mnist.train.labels, batch_size=32, nb_epoch=20, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9824/10000 [============================>.] - ETA: 0s  Loss:  0.0223456284106  , acc: 0.8922\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(mnist.test.images, mnist.test.labels);\n",
    "print('  Loss: ', scores[0], ' , acc:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical crosssentropy as the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5s - loss: 0.9081 - acc: 0.7441\n",
      "Epoch 2/20\n",
      "4s - loss: 0.3284 - acc: 0.9068\n",
      "Epoch 3/20\n",
      "5s - loss: 0.2736 - acc: 0.9211\n",
      "Epoch 4/20\n",
      "5s - loss: 0.2394 - acc: 0.9307\n",
      "Epoch 5/20\n",
      "5s - loss: 0.2132 - acc: 0.9387\n",
      "Epoch 6/20\n",
      "4s - loss: 0.1920 - acc: 0.9448\n",
      "Epoch 7/20\n",
      "5s - loss: 0.1749 - acc: 0.9492\n",
      "Epoch 8/20\n",
      "5s - loss: 0.1606 - acc: 0.9543\n",
      "Epoch 9/20\n",
      "4s - loss: 0.1482 - acc: 0.9571\n",
      "Epoch 10/20\n",
      "5s - loss: 0.1378 - acc: 0.9605\n",
      "Epoch 11/20\n",
      "5s - loss: 0.1287 - acc: 0.9632\n",
      "Epoch 12/20\n",
      "5s - loss: 0.1206 - acc: 0.9653\n",
      "Epoch 13/20\n",
      "4s - loss: 0.1136 - acc: 0.9678\n",
      "Epoch 14/20\n",
      "5s - loss: 0.1075 - acc: 0.9698\n",
      "Epoch 15/20\n",
      "5s - loss: 0.1016 - acc: 0.9711\n",
      "Epoch 16/20\n",
      "5s - loss: 0.0966 - acc: 0.9723\n",
      "Epoch 17/20\n",
      "5s - loss: 0.0920 - acc: 0.9744\n",
      "Epoch 18/20\n",
      "4s - loss: 0.0879 - acc: 0.9753\n",
      "Epoch 19/20\n",
      "4s - loss: 0.0836 - acc: 0.9771\n",
      "Epoch 20/20\n",
      "5s - loss: 0.0801 - acc: 0.9776\n",
      " 9856/10000 [============================>.] - ETA: 0s  Loss:  0.0946471564591  , acc: 0.9706\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100, input_dim=784, init='uniform', activation='sigmoid'))\n",
    "model.add(Dense(10, init='uniform', activation='sigmoid'))\n",
    "sgd = SGD(lr=0.1) \n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "model.fit(mnist.train.images, mnist.train.labels, batch_size=32, nb_epoch=20, verbose=2)\n",
    "scores = model.evaluate(mnist.test.images, mnist.test.labels);\n",
    "print('  Loss: ', scores[0], ' , acc:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**categorical_crossentropy** shows faster convergence. Thus we will stick on using **categorical_crossentropy** as loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Implement softmax to activation function in the output layers\n",
    "\n",
    "#### sigmoid activation -> softmax activation in the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "6s - loss: 0.7350 - acc: 0.7968\n",
      "Epoch 2/20\n",
      "5s - loss: 0.3181 - acc: 0.9087\n",
      "Epoch 3/20\n",
      "5s - loss: 0.2695 - acc: 0.9218\n",
      "Epoch 4/20\n",
      "6s - loss: 0.2369 - acc: 0.9322\n",
      "Epoch 5/20\n",
      "7s - loss: 0.2123 - acc: 0.9391\n",
      "Epoch 6/20\n",
      "6s - loss: 0.1915 - acc: 0.9447\n",
      "Epoch 7/20\n",
      "5s - loss: 0.1747 - acc: 0.9496\n",
      "Epoch 8/20\n",
      "5s - loss: 0.1605 - acc: 0.9540\n",
      "Epoch 9/20\n",
      "5s - loss: 0.1482 - acc: 0.9576\n",
      "Epoch 10/20\n",
      "5s - loss: 0.1376 - acc: 0.9605\n",
      "Epoch 11/20\n",
      "5s - loss: 0.1285 - acc: 0.9632\n",
      "Epoch 12/20\n",
      "5s - loss: 0.1204 - acc: 0.9654\n",
      "Epoch 13/20\n",
      "5s - loss: 0.1131 - acc: 0.9672\n",
      "Epoch 14/20\n",
      "5s - loss: 0.1068 - acc: 0.9695\n",
      "Epoch 15/20\n",
      "5s - loss: 0.1011 - acc: 0.9712\n",
      "Epoch 16/20\n",
      "5s - loss: 0.0962 - acc: 0.9730\n",
      "Epoch 17/20\n",
      "5s - loss: 0.0914 - acc: 0.9743\n",
      "Epoch 18/20\n",
      "5s - loss: 0.0871 - acc: 0.9757\n",
      "Epoch 19/20\n",
      "5s - loss: 0.0831 - acc: 0.9767\n",
      "Epoch 20/20\n",
      "5s - loss: 0.0795 - acc: 0.9779\n",
      " 9984/10000 [============================>.] - ETA: 0s  Loss:  0.0961759171788  , acc: 0.9702\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Dense(100, input_dim=784, init='uniform', activation='sigmoid'))\n",
    "model2.add(Dense(10, init='uniform', activation='softmax'))\n",
    "sgd = SGD(lr=0.1) \n",
    "model2.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "model2.fit(mnist.train.images, mnist.train.labels, batch_size=32, nb_epoch=20, verbose=2)\n",
    "scores = model2.evaluate(mnist.test.images, mnist.test.labels);\n",
    "print('  Loss: ', scores[0], ' , acc:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here it shows the selection of **sigmoid** and **softmax** in the output layer is only slightly relevant or even irrelevant to the model accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Enlarge the number of neurons in the hidden layer to 400\n",
    "\n",
    "#### number of neurons 100 -> 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "8s - loss: 0.6437 - acc: 0.8080\n",
      "Epoch 2/50\n",
      "8s - loss: 0.3286 - acc: 0.9044\n",
      "Epoch 3/50\n",
      "8s - loss: 0.2952 - acc: 0.9147\n",
      "Epoch 4/50\n",
      "8s - loss: 0.2705 - acc: 0.9210\n",
      "Epoch 5/50\n",
      "8s - loss: 0.2481 - acc: 0.9286\n",
      "Epoch 6/50\n",
      "8s - loss: 0.2266 - acc: 0.9346\n",
      "Epoch 7/50\n",
      "8s - loss: 0.2070 - acc: 0.9403\n",
      "Epoch 8/50\n",
      "8s - loss: 0.1895 - acc: 0.9461\n",
      "Epoch 9/50\n",
      "8s - loss: 0.1750 - acc: 0.9492\n",
      "Epoch 10/50\n",
      "8s - loss: 0.1612 - acc: 0.9544\n",
      "Epoch 11/50\n",
      "8s - loss: 0.1500 - acc: 0.9572\n",
      "Epoch 12/50\n",
      "8s - loss: 0.1395 - acc: 0.9601\n",
      "Epoch 13/50\n",
      "8s - loss: 0.1303 - acc: 0.9635\n",
      "Epoch 14/50\n",
      "8s - loss: 0.1219 - acc: 0.9656\n",
      "Epoch 15/50\n",
      "8s - loss: 0.1148 - acc: 0.9674\n",
      "Epoch 16/50\n",
      "8s - loss: 0.1082 - acc: 0.9693\n",
      "Epoch 17/50\n",
      "8s - loss: 0.1021 - acc: 0.9713\n",
      "Epoch 18/50\n",
      "8s - loss: 0.0967 - acc: 0.9730\n",
      "Epoch 19/50\n",
      "8s - loss: 0.0917 - acc: 0.9739\n",
      "Epoch 20/50\n",
      "8s - loss: 0.0869 - acc: 0.9758\n",
      "Epoch 21/50\n",
      "8s - loss: 0.0826 - acc: 0.9769\n",
      "Epoch 22/50\n",
      "8s - loss: 0.0787 - acc: 0.9782\n",
      "Epoch 23/50\n",
      "8s - loss: 0.0752 - acc: 0.9794\n",
      "Epoch 24/50\n",
      "8s - loss: 0.0715 - acc: 0.9804\n",
      "Epoch 25/50\n",
      "8s - loss: 0.0685 - acc: 0.9809\n",
      "Epoch 26/50\n",
      "8s - loss: 0.0657 - acc: 0.9818\n",
      "Epoch 27/50\n",
      "8s - loss: 0.0631 - acc: 0.9825\n",
      "Epoch 28/50\n",
      "8s - loss: 0.0604 - acc: 0.9831\n",
      "Epoch 29/50\n",
      "8s - loss: 0.0578 - acc: 0.9842\n",
      "Epoch 30/50\n",
      "8s - loss: 0.0555 - acc: 0.9848\n",
      "Epoch 31/50\n",
      "8s - loss: 0.0534 - acc: 0.9854\n",
      "Epoch 32/50\n",
      "8s - loss: 0.0513 - acc: 0.9859\n",
      "Epoch 33/50\n",
      "8s - loss: 0.0490 - acc: 0.9868\n",
      "Epoch 34/50\n",
      "8s - loss: 0.0473 - acc: 0.9874\n",
      "Epoch 35/50\n",
      "8s - loss: 0.0457 - acc: 0.9878\n",
      "Epoch 36/50\n",
      "8s - loss: 0.0440 - acc: 0.9885\n",
      "Epoch 37/50\n",
      "8s - loss: 0.0424 - acc: 0.9891\n",
      "Epoch 38/50\n",
      "8s - loss: 0.0409 - acc: 0.9895\n",
      "Epoch 39/50\n",
      "8s - loss: 0.0393 - acc: 0.9902\n",
      "Epoch 40/50\n",
      "8s - loss: 0.0380 - acc: 0.9899\n",
      "Epoch 41/50\n",
      "10s - loss: 0.0367 - acc: 0.9912\n",
      "Epoch 42/50\n",
      "9s - loss: 0.0353 - acc: 0.9913\n",
      "Epoch 43/50\n",
      "8s - loss: 0.0341 - acc: 0.9914\n",
      "Epoch 44/50\n",
      "9s - loss: 0.0329 - acc: 0.9919\n",
      "Epoch 45/50\n",
      "8s - loss: 0.0319 - acc: 0.9927\n",
      "Epoch 46/50\n",
      "10s - loss: 0.0307 - acc: 0.9928\n",
      "Epoch 47/50\n",
      "9s - loss: 0.0297 - acc: 0.9931\n",
      "Epoch 48/50\n",
      "9s - loss: 0.0288 - acc: 0.9933\n",
      "Epoch 49/50\n",
      "10s - loss: 0.0278 - acc: 0.9937\n",
      "Epoch 50/50\n",
      "9s - loss: 0.0268 - acc: 0.9945\n",
      "10000/10000 [==============================] - 1s     \n",
      "  Loss:  0.0671618557692  , acc: 0.9777\n"
     ]
    }
   ],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(Dense(400, input_dim=784, init='uniform', activation='sigmoid'))\n",
    "model3.add(Dense(10, init='uniform', activation='softmax'))\n",
    "sgd = SGD(lr=0.1) \n",
    "model3.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "model3.fit(mnist.train.images, mnist.train.labels, batch_size=32, nb_epoch=50, verbose=2)\n",
    "scores = model3.evaluate(mnist.test.images, mnist.test.labels);\n",
    "print('  Loss: ', scores[0], ' , acc:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously more nerons in the hidden layer is helpful! The training accuracy is enhanced to 99% though the model on the test sets still has similar performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Add more hidden layers: 3 hidden layers with 200-200-100 neurons\n",
    "#### one hidden layer -> 3 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "19s - loss: 2.3119 - acc: 0.1069\n",
      "Epoch 2/50\n",
      "16s - loss: 2.2996 - acc: 0.1193\n",
      "Epoch 3/50\n",
      "14s - loss: 1.9507 - acc: 0.2768\n",
      "Epoch 4/50\n",
      "16s - loss: 1.3733 - acc: 0.4822\n",
      "Epoch 5/50\n",
      "17s - loss: 0.7101 - acc: 0.7800\n",
      "Epoch 6/50\n",
      "17s - loss: 0.4131 - acc: 0.8841\n",
      "Epoch 7/50\n",
      "18s - loss: 0.2813 - acc: 0.9195\n",
      "Epoch 8/50\n",
      "15s - loss: 0.2248 - acc: 0.9360\n",
      "Epoch 9/50\n",
      "17s - loss: 0.1856 - acc: 0.9458\n",
      "Epoch 10/50\n",
      "16s - loss: 0.1579 - acc: 0.9538\n",
      "Epoch 11/50\n",
      "16s - loss: 0.1360 - acc: 0.9598\n",
      "Epoch 12/50\n",
      "15s - loss: 0.1188 - acc: 0.9653\n",
      "Epoch 13/50\n",
      "14s - loss: 0.1052 - acc: 0.9692\n",
      "Epoch 14/50\n",
      "14s - loss: 0.0937 - acc: 0.9723\n",
      "Epoch 15/50\n",
      "14s - loss: 0.0841 - acc: 0.9747\n",
      "Epoch 16/50\n",
      "14s - loss: 0.0765 - acc: 0.9771\n",
      "Epoch 17/50\n",
      "13s - loss: 0.0677 - acc: 0.9802\n",
      "Epoch 18/50\n",
      "14s - loss: 0.0621 - acc: 0.9816\n",
      "Epoch 19/50\n",
      "20s - loss: 0.0555 - acc: 0.9833\n",
      "Epoch 20/50\n",
      "16s - loss: 0.0510 - acc: 0.9845\n",
      "Epoch 21/50\n",
      "16s - loss: 0.0451 - acc: 0.9869\n",
      "Epoch 22/50\n",
      "15s - loss: 0.0410 - acc: 0.9879\n",
      "Epoch 23/50\n",
      "14s - loss: 0.0377 - acc: 0.9888\n",
      "Epoch 24/50\n",
      "15s - loss: 0.0340 - acc: 0.9897\n",
      "Epoch 25/50\n",
      "14s - loss: 0.0301 - acc: 0.9915\n",
      "Epoch 26/50\n",
      "14s - loss: 0.0256 - acc: 0.9929\n",
      "Epoch 27/50\n",
      "14s - loss: 0.0245 - acc: 0.9936\n",
      "Epoch 28/50\n",
      "14s - loss: 0.0220 - acc: 0.9940\n",
      "Epoch 29/50\n",
      "16s - loss: 0.0198 - acc: 0.9948\n",
      "Epoch 30/50\n",
      "14s - loss: 0.0178 - acc: 0.9955\n",
      "Epoch 31/50\n",
      "14s - loss: 0.0151 - acc: 0.9965\n",
      "Epoch 32/50\n",
      "16s - loss: 0.0136 - acc: 0.9968\n",
      "Epoch 33/50\n",
      "15s - loss: 0.0122 - acc: 0.9969\n",
      "Epoch 34/50\n",
      "16s - loss: 0.0105 - acc: 0.9977\n",
      "Epoch 35/50\n",
      "15s - loss: 0.0083 - acc: 0.9986\n",
      "Epoch 36/50\n",
      "17s - loss: 0.0077 - acc: 0.9986\n",
      "Epoch 37/50\n",
      "14s - loss: 0.0066 - acc: 0.9991\n",
      "Epoch 38/50\n",
      "16s - loss: 0.0060 - acc: 0.9991\n",
      "Epoch 39/50\n",
      "20s - loss: 0.0060 - acc: 0.9989\n",
      "Epoch 40/50\n",
      "16s - loss: 0.0046 - acc: 0.9995\n",
      "Epoch 41/50\n",
      "15s - loss: 0.0040 - acc: 0.9996\n",
      "Epoch 42/50\n",
      "14s - loss: 0.0035 - acc: 0.9998\n",
      "Epoch 43/50\n",
      "14s - loss: 0.0035 - acc: 0.9997\n",
      "Epoch 44/50\n",
      "18s - loss: 0.0031 - acc: 0.9998\n",
      "Epoch 45/50\n",
      "16s - loss: 0.0029 - acc: 0.9999\n",
      "Epoch 46/50\n",
      "15s - loss: 0.0027 - acc: 0.9998\n",
      "Epoch 47/50\n",
      "14s - loss: 0.0026 - acc: 0.9999\n",
      "Epoch 48/50\n",
      "14s - loss: 0.0024 - acc: 0.9999\n",
      "Epoch 49/50\n",
      "16s - loss: 0.0023 - acc: 0.9999\n",
      "Epoch 50/50\n",
      "15s - loss: 0.0023 - acc: 0.9999\n",
      " 9984/10000 [============================>.] - ETA: 0s  Loss:  0.0943529336645  , acc: 0.9784\n"
     ]
    }
   ],
   "source": [
    "model4 = Sequential()\n",
    "model4.add(Dense(200, input_dim=784, init='uniform', activation='sigmoid'))\n",
    "model4.add(Dense(200, init='uniform', activation='sigmoid'))\n",
    "model4.add(Dense(100, init='uniform', activation='sigmoid'))\n",
    "model4.add(Dense(10,  init='uniform', activation='softmax'))\n",
    "sgd = SGD(lr=0.1) \n",
    "model4.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "model4.fit(mnist.train.images, mnist.train.labels, batch_size=16, nb_epoch=50, verbose=2)\n",
    "scores = model4.evaluate(mnist.test.images, mnist.test.labels);\n",
    "print('  Loss: ', scores[0], ' , acc:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With more hidden layers, the trained model shows almost perfect accuracy (99.99%). For the test set, the model also shows to have accuracy of 98%. However, more hidden layers needs more iterations to get converegence. For example, at the beginning few epoches, the model accuracies are 0.1069 and 0.1193, which are worse than that using one hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e) Initialization\n",
    "\n",
    "#### Remove init = 'uniform'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "14s - loss: 1.0521 - acc: 0.6377\n",
      "Epoch 2/50\n",
      "14s - loss: 0.3380 - acc: 0.9008\n",
      "Epoch 3/50\n",
      "14s - loss: 0.2532 - acc: 0.9256\n",
      "Epoch 4/50\n",
      "14s - loss: 0.1984 - acc: 0.9411\n",
      "Epoch 5/50\n",
      "14s - loss: 0.1609 - acc: 0.9523\n",
      "Epoch 6/50\n",
      "14s - loss: 0.1370 - acc: 0.9585\n",
      "Epoch 7/50\n",
      "14s - loss: 0.1172 - acc: 0.9651\n",
      "Epoch 8/50\n",
      "14s - loss: 0.1027 - acc: 0.9688\n",
      "Epoch 9/50\n",
      "14s - loss: 0.0891 - acc: 0.9733\n",
      "Epoch 10/50\n",
      "14s - loss: 0.0800 - acc: 0.9763\n",
      "Epoch 11/50\n",
      "15s - loss: 0.0715 - acc: 0.9777\n",
      "Epoch 12/50\n",
      "15s - loss: 0.0639 - acc: 0.9802\n",
      "Epoch 13/50\n",
      "14s - loss: 0.0560 - acc: 0.9832\n",
      "Epoch 14/50\n",
      "14s - loss: 0.0501 - acc: 0.9850\n",
      "Epoch 15/50\n",
      "15s - loss: 0.0443 - acc: 0.9863\n",
      "Epoch 16/50\n",
      "20s - loss: 0.0392 - acc: 0.9882\n",
      "Epoch 17/50\n",
      "17s - loss: 0.0353 - acc: 0.9897\n",
      "Epoch 18/50\n",
      "17s - loss: 0.0313 - acc: 0.9908\n",
      "Epoch 19/50\n",
      "16s - loss: 0.0275 - acc: 0.9918\n",
      "Epoch 20/50\n",
      "23s - loss: 0.0252 - acc: 0.9922\n",
      "Epoch 21/50\n",
      "21s - loss: 0.0201 - acc: 0.9948\n",
      "Epoch 22/50\n",
      "14s - loss: 0.0188 - acc: 0.9944\n",
      "Epoch 23/50\n",
      "14s - loss: 0.0157 - acc: 0.9958\n",
      "Epoch 24/50\n",
      "15s - loss: 0.0142 - acc: 0.9962\n",
      "Epoch 25/50\n",
      "15s - loss: 0.0120 - acc: 0.9971\n",
      "Epoch 26/50\n",
      "13s - loss: 0.0104 - acc: 0.9975\n",
      "Epoch 27/50\n",
      "14s - loss: 0.0088 - acc: 0.9981\n",
      "Epoch 28/50\n",
      "16s - loss: 0.0076 - acc: 0.9987\n",
      "Epoch 29/50\n",
      "14s - loss: 0.0063 - acc: 0.9991\n",
      "Epoch 30/50\n",
      "14s - loss: 0.0056 - acc: 0.9991\n",
      "Epoch 31/50\n",
      "13s - loss: 0.0047 - acc: 0.9995\n",
      "Epoch 32/50\n",
      "14s - loss: 0.0041 - acc: 0.9997\n",
      "Epoch 33/50\n",
      "14s - loss: 0.0034 - acc: 0.9997\n",
      "Epoch 34/50\n",
      "14s - loss: 0.0031 - acc: 0.9999\n",
      "Epoch 35/50\n",
      "14s - loss: 0.0028 - acc: 0.9998\n",
      "Epoch 36/50\n",
      "14s - loss: 0.0025 - acc: 0.9999\n",
      "Epoch 37/50\n",
      "15s - loss: 0.0023 - acc: 1.0000\n",
      "Epoch 38/50\n",
      "15s - loss: 0.0022 - acc: 0.9999\n",
      "Epoch 39/50\n",
      "13s - loss: 0.0021 - acc: 0.9999\n",
      "Epoch 40/50\n",
      "14s - loss: 0.0019 - acc: 0.9999\n",
      "Epoch 41/50\n",
      "14s - loss: 0.0017 - acc: 0.9999\n",
      "Epoch 42/50\n",
      "14s - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 43/50\n",
      "15s - loss: 0.0015 - acc: 0.9999\n",
      "Epoch 44/50\n",
      "15s - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 45/50\n",
      "17s - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 46/50\n",
      "17s - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 47/50\n",
      "15s - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 48/50\n",
      "17s - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 49/50\n",
      "16s - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 50/50\n",
      "14s - loss: 0.0011 - acc: 1.0000\n",
      "10000/10000 [==============================] - 1s     \n",
      "  Loss:  0.0930763677595  , acc: 0.9801\n"
     ]
    }
   ],
   "source": [
    "model5 = Sequential()\n",
    "model5.add(Dense(200, input_dim=784, activation='sigmoid'))\n",
    "model5.add(Dense(200, activation='sigmoid'))\n",
    "model5.add(Dense(100, activation='sigmoid'))\n",
    "model5.add(Dense(10,  activation='softmax'))\n",
    "sgd = SGD(lr=0.1) \n",
    "model5.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "model5.fit(mnist.train.images, mnist.train.labels, batch_size=16, nb_epoch=50, verbose=2)\n",
    "scores = model5.evaluate(mnist.test.images, mnist.test.labels);\n",
    "print('  Loss: ', scores[0], ' , acc:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous neural network, the weight parameters were initialized by a uniform function. Here we generate the parameters using default initialization function in Keras, showing that the early epoches the model has better performance than one using uniform initialization function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (f) Implement relu activation in the hidden layers\n",
    "\n",
    "#### sigmoid activcation -> relu activaction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "15s - loss: 0.2487 - acc: 0.9227\n",
      "Epoch 2/30\n",
      "17s - loss: 0.1063 - acc: 0.9667\n",
      "Epoch 3/30\n",
      "17s - loss: 0.0752 - acc: 0.9770\n",
      "Epoch 4/30\n",
      "18s - loss: 0.0563 - acc: 0.9823\n",
      "Epoch 5/30\n",
      "19s - loss: 0.0436 - acc: 0.9862\n",
      "Epoch 6/30\n",
      "16s - loss: 0.0368 - acc: 0.9880\n",
      "Epoch 7/30\n",
      "16s - loss: 0.0309 - acc: 0.9903\n",
      "Epoch 8/30\n",
      "16s - loss: 0.0248 - acc: 0.9921\n",
      "Epoch 9/30\n",
      "18s - loss: 0.0187 - acc: 0.9939\n",
      "Epoch 10/30\n",
      "17s - loss: 0.0179 - acc: 0.9943\n",
      "Epoch 11/30\n",
      "14s - loss: 0.0200 - acc: 0.9934\n",
      "Epoch 12/30\n",
      "14s - loss: 0.0131 - acc: 0.9958\n",
      "Epoch 13/30\n",
      "13s - loss: 0.0151 - acc: 0.9951\n",
      "Epoch 14/30\n",
      "14s - loss: 0.0140 - acc: 0.9957\n",
      "Epoch 15/30\n",
      "13s - loss: 0.0104 - acc: 0.9968\n",
      "Epoch 16/30\n",
      "14s - loss: 0.0091 - acc: 0.9973\n",
      "Epoch 17/30\n",
      "14s - loss: 0.0124 - acc: 0.9960\n",
      "Epoch 18/30\n",
      "15s - loss: 0.0078 - acc: 0.9975\n",
      "Epoch 19/30\n",
      "15s - loss: 0.0133 - acc: 0.9957\n",
      "Epoch 20/30\n",
      "14s - loss: 0.0059 - acc: 0.9981\n",
      "Epoch 21/30\n",
      "13s - loss: 0.0048 - acc: 0.9984\n",
      "Epoch 22/30\n",
      "13s - loss: 0.0118 - acc: 0.9961\n",
      "Epoch 23/30\n",
      "13s - loss: 0.0115 - acc: 0.9967\n",
      "Epoch 24/30\n",
      "13s - loss: 0.0053 - acc: 0.9986\n",
      "Epoch 25/30\n",
      "13s - loss: 0.0033 - acc: 0.9989\n",
      "Epoch 26/30\n",
      "13s - loss: 6.9735e-04 - acc: 0.9998\n",
      "Epoch 27/30\n",
      "13s - loss: 0.0012 - acc: 0.9998\n",
      "Epoch 28/30\n",
      "14s - loss: 1.1801e-04 - acc: 1.0000\n",
      "Epoch 29/30\n",
      "18s - loss: 5.0030e-05 - acc: 1.0000\n",
      "Epoch 30/30\n",
      "17s - loss: 3.8850e-05 - acc: 1.0000\n",
      "10000/10000 [==============================] - 2s     \n",
      "  Loss:  0.0860895343051  , acc: 0.9837\n"
     ]
    }
   ],
   "source": [
    "model6 = Sequential()\n",
    "model6.add(Dense(200, input_dim=784, activation='relu'))\n",
    "model6.add(Dense(200, activation='relu'))\n",
    "model6.add(Dense(100, activation='relu'))\n",
    "model6.add(Dense(10,  activation='softmax'))\n",
    "sgd = SGD(lr=0.1) \n",
    "model6.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "model6.fit(mnist.train.images, mnist.train.labels, batch_size=16, nb_epoch=30, verbose=2)\n",
    "scores = model6.evaluate(mnist.test.images, mnist.test.labels);\n",
    "print('  Loss: ', scores[0], ' , acc:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using **relu** even converges faster, since in the previous case using sigmod activation function, the 100% accuracy happened at the 40-epoch, but here at 28-th epoch the model accuracy has been about 100%.\n",
    "\n",
    "#### From the benchmarks, the best network is given by multiple hidden layer. Using relu and default initialization show faster convergence for training the models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prediction\n",
    "\n",
    "**model.predict(X)** gives the probability of each class, and **model.predict_classes(X)** directly gives the class. The highest probability identifies the class which the image is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2.32193374e-21   2.14999036e-14   1.09257340e-14   3.19410599e-17\n",
      "    2.05686366e-14   3.64200596e-21   1.85665959e-21   1.00000000e+00\n",
      "    2.03881241e-16   9.47486989e-13]\n",
      " [  1.03341797e-14   3.32954316e-15   1.00000000e+00   2.59395833e-17\n",
      "    6.71541410e-19   7.84372000e-19   1.95160837e-18   8.02996953e-20\n",
      "    1.56550932e-16   1.12891557e-23]\n",
      " [  1.91516280e-16   1.00000000e+00   2.94681879e-12   2.92172170e-17\n",
      "    2.73391223e-11   5.84677706e-14   3.69494583e-13   3.03999556e-12\n",
      "    4.48173009e-12   6.35018906e-14]\n",
      " [  1.00000000e+00   9.61116974e-19   2.06171094e-10   5.55445116e-15\n",
      "    3.29830585e-15   3.67964021e-14   8.74113004e-11   1.01537196e-15\n",
      "    1.00236307e-17   4.42005721e-15]\n",
      " [  1.47901592e-14   1.17368206e-13   3.26535604e-11   5.74674334e-17\n",
      "    1.00000000e+00   1.10158731e-15   9.50973512e-14   9.46095112e-12\n",
      "    5.46443782e-15   1.31592415e-09]\n",
      " [  5.44018094e-17   1.00000000e+00   1.86798683e-14   8.85652687e-18\n",
      "    1.17272997e-11   1.75686227e-16   5.23730477e-16   1.85655935e-11\n",
      "    3.88071563e-12   1.39642150e-13]\n",
      " [  9.77386659e-16   1.24402114e-16   1.74720828e-12   1.10048631e-16\n",
      "    1.00000000e+00   5.13980622e-13   5.13642514e-13   5.97644018e-14\n",
      "    2.15244955e-09   7.24317273e-10]\n",
      " [  2.47072382e-13   1.05670579e-15   2.03300565e-13   3.60258383e-07\n",
      "    1.30923722e-12   4.00177003e-09   1.21468321e-17   1.84675799e-11\n",
      "    5.92950827e-11   9.99999642e-01]]\n"
     ]
    }
   ],
   "source": [
    "predictions = model5.predict(mnist.test.images)\n",
    "print (predictions[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s     \n",
      "[7 2 1 0 4 1 4 9]\n"
     ]
    }
   ],
   "source": [
    "predictions = model5.predict_classes(mnist.test.images)\n",
    "print (predictions[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeMAAAEkCAYAAAAcmlk0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1361b79e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotExamples(mnist.train.images[:8], mnist.train.labels[:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Revisit the \"mse\" loss function and uniform initialization\n",
    "\n",
    "Here we will revisit the model and study how important to choose a loss function. With **mse** as loss function, even in the same neural network, the convergence is always slower than using **categorical_crossentropy**. The uniform initialization further makes the model worse. However, by choosing **relu**, we can still improve the model performance.\n",
    "\n",
    "### Using \"mse\" loss function converges more slowly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "8s - loss: 0.0903 - acc: 0.1160\n",
      "Epoch 2/50\n",
      "8s - loss: 0.0899 - acc: 0.1168\n",
      "Epoch 3/50\n",
      "10s - loss: 0.0898 - acc: 0.1160\n",
      "Epoch 4/50\n",
      "9s - loss: 0.0898 - acc: 0.1189\n",
      "Epoch 5/50\n",
      "8s - loss: 0.0897 - acc: 0.1362\n",
      "Epoch 6/50\n",
      "7s - loss: 0.0896 - acc: 0.1361\n",
      "Epoch 7/50\n",
      "9s - loss: 0.0895 - acc: 0.1542\n",
      "Epoch 8/50\n",
      "9s - loss: 0.0894 - acc: 0.1612\n",
      "Epoch 9/50\n",
      "11s - loss: 0.0893 - acc: 0.1933\n",
      "Epoch 10/50\n",
      "10s - loss: 0.0891 - acc: 0.1980\n",
      "Epoch 11/50\n",
      "8s - loss: 0.0889 - acc: 0.2237\n",
      "Epoch 12/50\n",
      "9s - loss: 0.0886 - acc: 0.2191\n",
      "Epoch 13/50\n",
      "7s - loss: 0.0882 - acc: 0.2411\n",
      "Epoch 14/50\n",
      "8s - loss: 0.0876 - acc: 0.2248\n",
      "Epoch 15/50\n",
      "8s - loss: 0.0866 - acc: 0.2280\n",
      "Epoch 16/50\n",
      "8s - loss: 0.0846 - acc: 0.2289\n",
      "Epoch 17/50\n",
      "9s - loss: 0.0811 - acc: 0.2535\n",
      "Epoch 18/50\n",
      "8s - loss: 0.0770 - acc: 0.3193\n",
      "Epoch 19/50\n",
      "9s - loss: 0.0734 - acc: 0.3715\n",
      "Epoch 20/50\n",
      "8s - loss: 0.0702 - acc: 0.4072\n",
      "Epoch 21/50\n",
      "8s - loss: 0.0670 - acc: 0.4420\n",
      "Epoch 22/50\n",
      "10s - loss: 0.0640 - acc: 0.4829\n",
      "Epoch 23/50\n",
      "11s - loss: 0.0609 - acc: 0.5285\n",
      "Epoch 24/50\n",
      "9s - loss: 0.0574 - acc: 0.5790\n",
      "Epoch 25/50\n",
      "8s - loss: 0.0535 - acc: 0.6294\n",
      "Epoch 26/50\n",
      "8s - loss: 0.0492 - acc: 0.6748\n",
      "Epoch 27/50\n",
      "7s - loss: 0.0450 - acc: 0.7150\n",
      "Epoch 28/50\n",
      "9s - loss: 0.0412 - acc: 0.7446\n",
      "Epoch 29/50\n",
      "8s - loss: 0.0380 - acc: 0.7675\n",
      "Epoch 30/50\n",
      "7s - loss: 0.0353 - acc: 0.7867\n",
      "Epoch 31/50\n",
      "7s - loss: 0.0329 - acc: 0.8040\n",
      "Epoch 32/50\n",
      "7s - loss: 0.0308 - acc: 0.8167\n",
      "Epoch 33/50\n",
      "7s - loss: 0.0288 - acc: 0.8295\n",
      "Epoch 34/50\n",
      "7s - loss: 0.0271 - acc: 0.8402\n",
      "Epoch 35/50\n",
      "7s - loss: 0.0256 - acc: 0.8486\n",
      "Epoch 36/50\n",
      "8s - loss: 0.0242 - acc: 0.8557\n",
      "Epoch 37/50\n",
      "7s - loss: 0.0230 - acc: 0.8618\n",
      "Epoch 38/50\n",
      "7s - loss: 0.0220 - acc: 0.8671\n",
      "Epoch 39/50\n",
      "9s - loss: 0.0210 - acc: 0.8714\n",
      "Epoch 40/50\n",
      "8s - loss: 0.0203 - acc: 0.8752\n",
      "Epoch 41/50\n",
      "8s - loss: 0.0196 - acc: 0.8791\n",
      "Epoch 42/50\n",
      "9s - loss: 0.0190 - acc: 0.8824\n",
      "Epoch 43/50\n",
      "7s - loss: 0.0185 - acc: 0.8846\n",
      "Epoch 44/50\n",
      "7s - loss: 0.0180 - acc: 0.8870\n",
      "Epoch 45/50\n",
      "8s - loss: 0.0176 - acc: 0.8892\n",
      "Epoch 46/50\n",
      "9s - loss: 0.0173 - acc: 0.8913\n",
      "Epoch 47/50\n",
      "9s - loss: 0.0169 - acc: 0.8929\n",
      "Epoch 48/50\n",
      "8s - loss: 0.0167 - acc: 0.8944\n",
      "Epoch 49/50\n",
      "8s - loss: 0.0164 - acc: 0.8959\n",
      "Epoch 50/50\n",
      "8s - loss: 0.0161 - acc: 0.8973\n",
      " 9920/10000 [============================>.] - ETA: 0s  Loss:  0.0156185688918  , acc: 0.8984\n"
     ]
    }
   ],
   "source": [
    "model7 = Sequential()\n",
    "model7.add(Dense(200, input_dim=784, activation='sigmoid'))\n",
    "model7.add(Dense(200, activation='sigmoid'))\n",
    "model7.add(Dense(100, activation='sigmoid'))\n",
    "model7.add(Dense(10, activation='softmax'))\n",
    "sgd = SGD(lr=0.1) \n",
    "model7.compile(loss='mse', optimizer=sgd, metrics=['accuracy'])\n",
    "model7.fit(mnist.train.images, mnist.train.labels, batch_size=32, nb_epoch=50, verbose=2)\n",
    "scores = model7.evaluate(mnist.test.images, mnist.test.labels);\n",
    "print('  Loss: ', scores[0], ' , acc:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using uniform initialization + mse loss shows poor performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "8s - loss: 0.0900 - acc: 0.1095\n",
      "Epoch 2/30\n",
      "7s - loss: 0.0900 - acc: 0.1115\n",
      "Epoch 3/30\n",
      "7s - loss: 0.0900 - acc: 0.1117\n",
      "Epoch 4/30\n",
      "7s - loss: 0.0900 - acc: 0.1114\n",
      "Epoch 5/30\n",
      "8s - loss: 0.0900 - acc: 0.1117\n",
      "Epoch 6/30\n",
      "10s - loss: 0.0900 - acc: 0.1117\n",
      "Epoch 7/30\n",
      "8s - loss: 0.0900 - acc: 0.1119\n",
      "Epoch 8/30\n",
      "9s - loss: 0.0900 - acc: 0.1121\n",
      "Epoch 9/30\n",
      "10s - loss: 0.0900 - acc: 0.1117\n",
      "Epoch 10/30\n",
      "7s - loss: 0.0900 - acc: 0.1123\n",
      "Epoch 11/30\n",
      "7s - loss: 0.0900 - acc: 0.1113\n",
      "Epoch 12/30\n",
      "10s - loss: 0.0900 - acc: 0.1120\n",
      "Epoch 13/30\n",
      "10s - loss: 0.0900 - acc: 0.1120\n",
      "Epoch 14/30\n",
      "10s - loss: 0.0900 - acc: 0.1123\n",
      "Epoch 15/30\n",
      "9s - loss: 0.0900 - acc: 0.1121\n",
      "Epoch 16/30\n",
      "7s - loss: 0.0900 - acc: 0.1123\n",
      "Epoch 17/30\n",
      "8s - loss: 0.0900 - acc: 0.1120\n",
      "Epoch 18/30\n",
      "8s - loss: 0.0900 - acc: 0.1117\n",
      "Epoch 19/30\n",
      "9s - loss: 0.0900 - acc: 0.1119\n",
      "Epoch 20/30\n",
      "8s - loss: 0.0900 - acc: 0.1122\n",
      "Epoch 21/30\n",
      "7s - loss: 0.0900 - acc: 0.1123\n",
      "Epoch 22/30\n",
      "11s - loss: 0.0900 - acc: 0.1123\n",
      "Epoch 23/30\n",
      "14s - loss: 0.0900 - acc: 0.1123\n",
      "Epoch 24/30\n",
      "12s - loss: 0.0900 - acc: 0.1123\n",
      "Epoch 25/30\n",
      "9s - loss: 0.0900 - acc: 0.1123\n",
      "Epoch 26/30\n",
      "8s - loss: 0.0900 - acc: 0.1123\n",
      "Epoch 27/30\n",
      "7s - loss: 0.0900 - acc: 0.1120\n",
      "Epoch 28/30\n",
      "7s - loss: 0.0900 - acc: 0.1119\n",
      "Epoch 29/30\n",
      "7s - loss: 0.0900 - acc: 0.1120\n",
      "Epoch 30/30\n",
      "7s - loss: 0.0900 - acc: 0.1119\n",
      " 9888/10000 [============================>.] - ETA: 0s  Loss:  0.0899695995569  , acc: 0.1135\n"
     ]
    }
   ],
   "source": [
    "model8 = Sequential()\n",
    "model8.add(Dense(200, input_dim=784, init='uniform', activation='sigmoid'))\n",
    "model8.add(Dense(200, init='uniform', activation='sigmoid'))\n",
    "model8.add(Dense(100, init='uniform', activation='sigmoid'))\n",
    "model8.add(Dense(10, init='uniform', activation='softmax'))\n",
    "sgd = SGD(lr=0.1) \n",
    "model8.compile(loss='mse', optimizer=sgd, metrics=['accuracy'])\n",
    "model8.fit(mnist.train.images, mnist.train.labels, batch_size=32, nb_epoch=30, verbose=2)\n",
    "scores = model8.evaluate(mnist.test.images, mnist.test.labels);\n",
    "print('  Loss: ', scores[0], ' , acc:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using relu activaction function slightly better than using sigmoid\n",
    "\n",
    "In the previous case, we saw the trained model got stuck to have low accuracy. However, using **relu** is helpful.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "8s - loss: 0.0900 - acc: 0.1209\n",
      "Epoch 2/50\n",
      "8s - loss: 0.0899 - acc: 0.1173\n",
      "Epoch 3/50\n",
      "9s - loss: 0.0897 - acc: 0.1925\n",
      "Epoch 4/50\n",
      "9s - loss: 0.0892 - acc: 0.2190\n",
      "Epoch 5/50\n",
      "7s - loss: 0.0848 - acc: 0.2252\n",
      "Epoch 6/50\n",
      "7s - loss: 0.0761 - acc: 0.3729\n",
      "Epoch 7/50\n",
      "7s - loss: 0.0491 - acc: 0.6571\n",
      "Epoch 8/50\n",
      "7s - loss: 0.0268 - acc: 0.8260\n",
      "Epoch 9/50\n",
      "7s - loss: 0.0208 - acc: 0.8652\n",
      "Epoch 10/50\n",
      "7s - loss: 0.0181 - acc: 0.8823\n",
      "Epoch 11/50\n",
      "7s - loss: 0.0164 - acc: 0.8934\n",
      "Epoch 12/50\n",
      "8s - loss: 0.0150 - acc: 0.9032\n",
      "Epoch 13/50\n",
      "7s - loss: 0.0137 - acc: 0.9114\n",
      "Epoch 14/50\n",
      "10s - loss: 0.0127 - acc: 0.9188\n",
      "Epoch 15/50\n",
      "8s - loss: 0.0116 - acc: 0.9256\n",
      "Epoch 16/50\n",
      "8s - loss: 0.0107 - acc: 0.9314\n",
      "Epoch 17/50\n",
      "8s - loss: 0.0099 - acc: 0.9379\n",
      "Epoch 18/50\n",
      "7s - loss: 0.0092 - acc: 0.9415\n",
      "Epoch 19/50\n",
      "7s - loss: 0.0086 - acc: 0.9461\n",
      "Epoch 20/50\n",
      "7s - loss: 0.0080 - acc: 0.9495\n",
      "Epoch 21/50\n",
      "7s - loss: 0.0075 - acc: 0.9527\n",
      "Epoch 22/50\n",
      "7s - loss: 0.0071 - acc: 0.9553\n",
      "Epoch 23/50\n",
      "8s - loss: 0.0066 - acc: 0.9589\n",
      "Epoch 24/50\n",
      "7s - loss: 0.0063 - acc: 0.9619\n",
      "Epoch 25/50\n",
      "7s - loss: 0.0059 - acc: 0.9639\n",
      "Epoch 26/50\n",
      "7s - loss: 0.0056 - acc: 0.9663\n",
      "Epoch 27/50\n",
      "8s - loss: 0.0053 - acc: 0.9678\n",
      "Epoch 28/50\n",
      "8s - loss: 0.0051 - acc: 0.9693\n",
      "Epoch 29/50\n",
      "8s - loss: 0.0049 - acc: 0.9706\n",
      "Epoch 30/50\n",
      "7s - loss: 0.0046 - acc: 0.9726\n",
      "Epoch 31/50\n",
      "8s - loss: 0.0044 - acc: 0.9735\n",
      "Epoch 32/50\n",
      "8s - loss: 0.0042 - acc: 0.9748\n",
      "Epoch 33/50\n",
      "7s - loss: 0.0040 - acc: 0.9764\n",
      "Epoch 34/50\n",
      "7s - loss: 0.0039 - acc: 0.9775\n",
      "Epoch 35/50\n",
      "7s - loss: 0.0037 - acc: 0.9786\n",
      "Epoch 36/50\n",
      "7s - loss: 0.0036 - acc: 0.9789\n",
      "Epoch 37/50\n",
      "7s - loss: 0.0034 - acc: 0.9803\n",
      "Epoch 38/50\n",
      "7s - loss: 0.0033 - acc: 0.9817\n",
      "Epoch 39/50\n",
      "7s - loss: 0.0032 - acc: 0.9814\n",
      "Epoch 40/50\n",
      "7s - loss: 0.0031 - acc: 0.9828\n",
      "Epoch 41/50\n",
      "7s - loss: 0.0030 - acc: 0.9835\n",
      "Epoch 42/50\n",
      "7s - loss: 0.0029 - acc: 0.9840\n",
      "Epoch 43/50\n",
      "7s - loss: 0.0027 - acc: 0.9850\n",
      "Epoch 44/50\n",
      "7s - loss: 0.0027 - acc: 0.9854\n",
      "Epoch 45/50\n",
      "7s - loss: 0.0026 - acc: 0.9861\n",
      "Epoch 46/50\n",
      "7s - loss: 0.0025 - acc: 0.9866\n",
      "Epoch 47/50\n",
      "7s - loss: 0.0024 - acc: 0.9873\n",
      "Epoch 48/50\n",
      "7s - loss: 0.0023 - acc: 0.9875\n",
      "Epoch 49/50\n",
      "7s - loss: 0.0022 - acc: 0.9882\n",
      "Epoch 50/50\n",
      "7s - loss: 0.0022 - acc: 0.9891\n",
      " 9888/10000 [============================>.] - ETA: 0s  Loss:  0.00430187083261  , acc: 0.9728\n"
     ]
    }
   ],
   "source": [
    "model9 = Sequential()\n",
    "model9.add(Dense(200, input_dim=784, init='uniform', activation='relu'))\n",
    "model9.add(Dense(200, init='uniform', activation='relu'))\n",
    "model9.add(Dense(100, init='uniform', activation='relu'))\n",
    "model9.add(Dense(10, init='uniform', activation='softmax'))\n",
    "sgd = SGD(lr=0.1) \n",
    "model9.compile(loss='mse', optimizer=sgd, metrics=['accuracy'])\n",
    "model9.fit(mnist.train.images, mnist.train.labels, batch_size=32, nb_epoch=50, verbose=2)\n",
    "scores = model9.evaluate(mnist.test.images, mnist.test.labels);\n",
    "print('  Loss: ', scores[0], ' , acc:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The best neural network is given by the 3-hidden-layer network structure 784-200-200-100-10, and using **relu** as activation function in the hidden layers and **softmax** in the output layer as well as the default initialization to train the models. For the multilcass classification, **categorical_crossentropy** loss function is preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
