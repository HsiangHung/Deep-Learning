{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Dataset-1: Recognition of Handwritten Digits\n",
    "\n",
    "In this note, we will implement Keras to build neural network models to recognize handwritten digits using **mnist** data. Here my backend is **tensorflow**. We will show that by properly choosing the network structures and parameters, the best model trained can have accuracy close to 100% and 98% on the test dataset. Through the experiments, the best neural network shown is given by a three-hidden-layer network using **relu** activaction function. Moreover, the model performance and tranining accuracy is sensitive to choice of parameters initialization and loss functions.\n",
    "\n",
    "We fetch mnist data from Yann LeCun's website. Using this dataset, the learning rate $\\alpha$ is not irrelevant to the model performance and accuracy. We stick on using $\\alpha$ = 0.1. However, we will see in the other note, mnist_digits_recognition-2, while using data from Keras, the learning rate is important!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. The MNIST database\n",
    "\n",
    "The handwritten digits data can be obtained from [Yann LeCun's website](http://yann.lecun.com/exdb/mnist/). Each image is 28 pixels by 28 pixels and is labeled 0-9 to identify the digits 0-9. This is a 10-class classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.contrib.learn.python.learn.datasets.base.Datasets"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 55000 samples in training datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((55000, 784), (55000, 10))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images.shape, mnist.train.labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10000 samples in test datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 784), (10000, 10))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.test.images.shape, mnist.test.labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Each MNIST image is represented in a784-dimensional vector space with real number [0,1]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.38039219,  0.37647063,  0.3019608 ,\n",
       "        0.46274513,  0.2392157 ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.35294119,  0.5411765 ,  0.92156869,\n",
       "        0.92156869,  0.92156869,  0.92156869,  0.92156869,  0.92156869,\n",
       "        0.98431379,  0.98431379,  0.97254908,  0.99607849,  0.96078438,\n",
       "        0.92156869,  0.74509805,  0.08235294,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.54901963,\n",
       "        0.98431379,  0.99607849,  0.99607849,  0.99607849,  0.99607849,\n",
       "        0.99607849,  0.99607849,  0.99607849,  0.99607849,  0.99607849,\n",
       "        0.99607849,  0.99607849,  0.99607849,  0.99607849,  0.99607849,\n",
       "        0.74117649,  0.09019608,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.88627458,  0.99607849,  0.81568635,\n",
       "        0.78039223,  0.78039223,  0.78039223,  0.78039223,  0.54509807,\n",
       "        0.2392157 ,  0.2392157 ,  0.2392157 ,  0.2392157 ,  0.2392157 ,\n",
       "        0.50196081,  0.8705883 ,  0.99607849,  0.99607849,  0.74117649,\n",
       "        0.08235294,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.14901961,  0.32156864,  0.0509804 ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.13333334,\n",
       "        0.83529419,  0.99607849,  0.99607849,  0.45098042,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.32941177,  0.99607849,\n",
       "        0.99607849,  0.91764712,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.32941177,  0.99607849,  0.99607849,  0.91764712,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.41568631,  0.6156863 ,\n",
       "        0.99607849,  0.99607849,  0.95294124,  0.20000002,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.09803922,  0.45882356,  0.89411771,  0.89411771,\n",
       "        0.89411771,  0.99215692,  0.99607849,  0.99607849,  0.99607849,\n",
       "        0.99607849,  0.94117653,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.26666668,  0.4666667 ,  0.86274517,\n",
       "        0.99607849,  0.99607849,  0.99607849,  0.99607849,  0.99607849,\n",
       "        0.99607849,  0.99607849,  0.99607849,  0.99607849,  0.55686277,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.14509805,  0.73333335,\n",
       "        0.99215692,  0.99607849,  0.99607849,  0.99607849,  0.87450987,\n",
       "        0.80784321,  0.80784321,  0.29411766,  0.26666668,  0.84313732,\n",
       "        0.99607849,  0.99607849,  0.45882356,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.44313729,  0.8588236 ,  0.99607849,  0.94901967,  0.89019614,\n",
       "        0.45098042,  0.34901962,  0.12156864,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.7843138 ,  0.99607849,  0.9450981 ,\n",
       "        0.16078432,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.66274512,  0.99607849,\n",
       "        0.6901961 ,  0.24313727,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.18823531,\n",
       "        0.90588242,  0.99607849,  0.91764712,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.07058824,  0.48627454,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.32941177,  0.99607849,  0.99607849,\n",
       "        0.65098041,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.54509807,  0.99607849,  0.9333334 ,  0.22352943,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.82352948,  0.98039222,  0.99607849,\n",
       "        0.65882355,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.94901967,  0.99607849,  0.93725497,  0.22352943,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.34901962,  0.98431379,  0.9450981 ,\n",
       "        0.33725491,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.01960784,\n",
       "        0.80784321,  0.96470594,  0.6156863 ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.01568628,  0.45882356,  0.27058825,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, tuple)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mnist.train.images), type(mnist.train.labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotExamples(data, labels):\n",
    "    plt.figure(figsize=(8,5))\n",
    "    for i in range(8):\n",
    "        sub = 241 + i\n",
    "        ax = plt.subplot(sub)\n",
    "        index = np.random.randint(0, data.shape[0])\n",
    "        ax.set_title(\"num: \" + str(np.argmax(labels[index])))\n",
    "        im = np.reshape(data[index], (28, 28))\n",
    "        plt.imshow(im, cmap=\"gray\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeMAAAEkCAYAAAAcmlk0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1250fe908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotExamples(mnist.train.images, mnist.train.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Training Neural Networks using Keras\n",
    "\n",
    "Here we consider different neural network structures to train models. We compare the models by different number of neurons in hidden layers, different loss functions: **mse** and **categorical_crossentropy**, different activation functions: **sigoid** and **relu**, more hidden layers and even initialization. The neural network in the **mnist** data always has the input layer with 784 inputs and the output layer with 10 neurons (10 classes). \n",
    "\n",
    "Let us start with the neural network: one hidden layer with 100 neurons, using **sigmoid** as activation function in the hidden layer and output layer, and uniform initialization. For each variant, we will reveal how the structure infleunce the training results by comparing model performance. \n",
    "\n",
    "### (a) Initial network: 1-hidden layer with 100 neurons, sigmoid activation in all layers\n",
    "\n",
    "At the beginning we compare using **mse** and **categorical_crossentropy** as loss functions. The result shows using **categorical_crossentropy** as loss functions has faster convergence, so we will stick on using **categorical_crossentropy**. Later we will revisit using **mse**.\n",
    "\n",
    "#### Mean squared error (mse) as the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "4s - loss: 0.0916 - acc: 0.1263\n",
      "Epoch 2/20\n",
      "4s - loss: 0.0886 - acc: 0.2060\n",
      "Epoch 3/20\n",
      "4s - loss: 0.0861 - acc: 0.2575\n",
      "Epoch 4/20\n",
      "4s - loss: 0.0798 - acc: 0.3408\n",
      "Epoch 5/20\n",
      "4s - loss: 0.0715 - acc: 0.4701\n",
      "Epoch 6/20\n",
      "4s - loss: 0.0631 - acc: 0.5856\n",
      "Epoch 7/20\n",
      "4s - loss: 0.0557 - acc: 0.6770\n",
      "Epoch 8/20\n",
      "4s - loss: 0.0497 - acc: 0.7303\n",
      "Epoch 9/20\n",
      "4s - loss: 0.0447 - acc: 0.7681\n",
      "Epoch 10/20\n",
      "4s - loss: 0.0408 - acc: 0.7937\n",
      "Epoch 11/20\n",
      "4s - loss: 0.0377 - acc: 0.8150\n",
      "Epoch 12/20\n",
      "4s - loss: 0.0351 - acc: 0.8321\n",
      "Epoch 13/20\n",
      "4s - loss: 0.0330 - acc: 0.8453\n",
      "Epoch 14/20\n",
      "4s - loss: 0.0312 - acc: 0.8553\n",
      "Epoch 15/20\n",
      "4s - loss: 0.0297 - acc: 0.8617\n",
      "Epoch 16/20\n",
      "4s - loss: 0.0283 - acc: 0.8675\n",
      "Epoch 17/20\n",
      "4s - loss: 0.0271 - acc: 0.8724\n",
      "Epoch 18/20\n",
      "4s - loss: 0.0261 - acc: 0.8761\n",
      "Epoch 19/20\n",
      "4s - loss: 0.0252 - acc: 0.8785\n",
      "Epoch 20/20\n",
      "4s - loss: 0.0244 - acc: 0.8816\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x132f2da58>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100, input_dim=784, init='uniform', activation='sigmoid'))\n",
    "model.add(Dense(10, init='uniform', activation='sigmoid'))\n",
    "## lr is the learning rate\n",
    "sgd = SGD(lr=0.1) \n",
    "model.compile(loss='mse', optimizer=sgd, metrics=['accuracy'])\n",
    "model.fit(mnist.train.images, mnist.train.labels, batch_size=32, nb_epoch=20, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9856/10000 [============================>.] - ETA: 0s  Loss:  0.0229811948523  , acc: 0.8893\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(mnist.test.images, mnist.test.labels);\n",
    "print('  Loss: ', scores[0], ' , acc:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical crosssentropy as the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5s - loss: 0.8876 - acc: 0.7504\n",
      "Epoch 2/20\n",
      "5s - loss: 0.3267 - acc: 0.9069\n",
      "Epoch 3/20\n",
      "4s - loss: 0.2731 - acc: 0.9215\n",
      "Epoch 4/20\n",
      "5s - loss: 0.2402 - acc: 0.9301\n",
      "Epoch 5/20\n",
      "5s - loss: 0.2144 - acc: 0.9383\n",
      "Epoch 6/20\n",
      "4s - loss: 0.1940 - acc: 0.9441\n",
      "Epoch 7/20\n",
      "4s - loss: 0.1770 - acc: 0.9482\n",
      "Epoch 8/20\n",
      "5s - loss: 0.1625 - acc: 0.9532\n",
      "Epoch 9/20\n",
      "5s - loss: 0.1507 - acc: 0.9564\n",
      "Epoch 10/20\n",
      "6s - loss: 0.1399 - acc: 0.9605\n",
      "Epoch 11/20\n",
      "7s - loss: 0.1309 - acc: 0.9621\n",
      "Epoch 12/20\n",
      "6s - loss: 0.1227 - acc: 0.9650\n",
      "Epoch 13/20\n",
      "6s - loss: 0.1153 - acc: 0.9670\n",
      "Epoch 14/20\n",
      "6s - loss: 0.1090 - acc: 0.9690\n",
      "Epoch 15/20\n",
      "6s - loss: 0.1031 - acc: 0.9711\n",
      "Epoch 16/20\n",
      "5s - loss: 0.0980 - acc: 0.9724\n",
      "Epoch 17/20\n",
      "5s - loss: 0.0931 - acc: 0.9739\n",
      "Epoch 18/20\n",
      "5s - loss: 0.0885 - acc: 0.9757\n",
      "Epoch 19/20\n",
      "5s - loss: 0.0846 - acc: 0.9764\n",
      "Epoch 20/20\n",
      "5s - loss: 0.0809 - acc: 0.9780\n",
      " 9920/10000 [============================>.] - ETA: 0s  Loss:  0.0961783868916  , acc: 0.9709\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100, input_dim=784, init='uniform', activation='sigmoid'))\n",
    "model.add(Dense(10, init='uniform', activation='sigmoid'))\n",
    "sgd = SGD(lr=0.1) \n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "model.fit(mnist.train.images, mnist.train.labels, batch_size=32, nb_epoch=20, verbose=2)\n",
    "scores = model.evaluate(mnist.test.images, mnist.test.labels);\n",
    "print('  Loss: ', scores[0], ' , acc:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**categorical_crossentropy** shows faster convergence. Thus we will stick on using **categorical_crossentropy** as loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Implement softmax to activation function in the output layers\n",
    "\n",
    "#### sigmoid activation -> softmax activation in the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "6s - loss: 0.7350 - acc: 0.7968\n",
      "Epoch 2/20\n",
      "5s - loss: 0.3181 - acc: 0.9087\n",
      "Epoch 3/20\n",
      "5s - loss: 0.2695 - acc: 0.9218\n",
      "Epoch 4/20\n",
      "6s - loss: 0.2369 - acc: 0.9322\n",
      "Epoch 5/20\n",
      "7s - loss: 0.2123 - acc: 0.9391\n",
      "Epoch 6/20\n",
      "6s - loss: 0.1915 - acc: 0.9447\n",
      "Epoch 7/20\n",
      "5s - loss: 0.1747 - acc: 0.9496\n",
      "Epoch 8/20\n",
      "5s - loss: 0.1605 - acc: 0.9540\n",
      "Epoch 9/20\n",
      "5s - loss: 0.1482 - acc: 0.9576\n",
      "Epoch 10/20\n",
      "5s - loss: 0.1376 - acc: 0.9605\n",
      "Epoch 11/20\n",
      "5s - loss: 0.1285 - acc: 0.9632\n",
      "Epoch 12/20\n",
      "5s - loss: 0.1204 - acc: 0.9654\n",
      "Epoch 13/20\n",
      "5s - loss: 0.1131 - acc: 0.9672\n",
      "Epoch 14/20\n",
      "5s - loss: 0.1068 - acc: 0.9695\n",
      "Epoch 15/20\n",
      "5s - loss: 0.1011 - acc: 0.9712\n",
      "Epoch 16/20\n",
      "5s - loss: 0.0962 - acc: 0.9730\n",
      "Epoch 17/20\n",
      "5s - loss: 0.0914 - acc: 0.9743\n",
      "Epoch 18/20\n",
      "5s - loss: 0.0871 - acc: 0.9757\n",
      "Epoch 19/20\n",
      "5s - loss: 0.0831 - acc: 0.9767\n",
      "Epoch 20/20\n",
      "5s - loss: 0.0795 - acc: 0.9779\n",
      " 9984/10000 [============================>.] - ETA: 0s  Loss:  0.0961759171788  , acc: 0.9702\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Dense(100, input_dim=784, init='uniform', activation='sigmoid'))\n",
    "model2.add(Dense(10, init='uniform', activation='softmax'))\n",
    "sgd = SGD(lr=0.1) \n",
    "model2.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "model2.fit(mnist.train.images, mnist.train.labels, batch_size=32, nb_epoch=20, verbose=2)\n",
    "scores = model2.evaluate(mnist.test.images, mnist.test.labels);\n",
    "print('  Loss: ', scores[0], ' , acc:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here it shows the selection of **sigmoid** and **softmax** in the output layer is only slightly relevant or even irrelevant to the model accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Enlarge the number of neurons in the hidden layer to 400\n",
    "\n",
    "#### number of neurons 100 -> 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "8s - loss: 0.6437 - acc: 0.8080\n",
      "Epoch 2/50\n",
      "8s - loss: 0.3286 - acc: 0.9044\n",
      "Epoch 3/50\n",
      "8s - loss: 0.2952 - acc: 0.9147\n",
      "Epoch 4/50\n",
      "8s - loss: 0.2705 - acc: 0.9210\n",
      "Epoch 5/50\n",
      "8s - loss: 0.2481 - acc: 0.9286\n",
      "Epoch 6/50\n",
      "8s - loss: 0.2266 - acc: 0.9346\n",
      "Epoch 7/50\n",
      "8s - loss: 0.2070 - acc: 0.9403\n",
      "Epoch 8/50\n",
      "8s - loss: 0.1895 - acc: 0.9461\n",
      "Epoch 9/50\n",
      "8s - loss: 0.1750 - acc: 0.9492\n",
      "Epoch 10/50\n",
      "8s - loss: 0.1612 - acc: 0.9544\n",
      "Epoch 11/50\n",
      "8s - loss: 0.1500 - acc: 0.9572\n",
      "Epoch 12/50\n",
      "8s - loss: 0.1395 - acc: 0.9601\n",
      "Epoch 13/50\n",
      "8s - loss: 0.1303 - acc: 0.9635\n",
      "Epoch 14/50\n",
      "8s - loss: 0.1219 - acc: 0.9656\n",
      "Epoch 15/50\n",
      "8s - loss: 0.1148 - acc: 0.9674\n",
      "Epoch 16/50\n",
      "8s - loss: 0.1082 - acc: 0.9693\n",
      "Epoch 17/50\n",
      "8s - loss: 0.1021 - acc: 0.9713\n",
      "Epoch 18/50\n",
      "8s - loss: 0.0967 - acc: 0.9730\n",
      "Epoch 19/50\n",
      "8s - loss: 0.0917 - acc: 0.9739\n",
      "Epoch 20/50\n",
      "8s - loss: 0.0869 - acc: 0.9758\n",
      "Epoch 21/50\n",
      "8s - loss: 0.0826 - acc: 0.9769\n",
      "Epoch 22/50\n",
      "8s - loss: 0.0787 - acc: 0.9782\n",
      "Epoch 23/50\n",
      "8s - loss: 0.0752 - acc: 0.9794\n",
      "Epoch 24/50\n",
      "8s - loss: 0.0715 - acc: 0.9804\n",
      "Epoch 25/50\n",
      "8s - loss: 0.0685 - acc: 0.9809\n",
      "Epoch 26/50\n",
      "8s - loss: 0.0657 - acc: 0.9818\n",
      "Epoch 27/50\n",
      "8s - loss: 0.0631 - acc: 0.9825\n",
      "Epoch 28/50\n",
      "8s - loss: 0.0604 - acc: 0.9831\n",
      "Epoch 29/50\n",
      "8s - loss: 0.0578 - acc: 0.9842\n",
      "Epoch 30/50\n",
      "8s - loss: 0.0555 - acc: 0.9848\n",
      "Epoch 31/50\n",
      "8s - loss: 0.0534 - acc: 0.9854\n",
      "Epoch 32/50\n",
      "8s - loss: 0.0513 - acc: 0.9859\n",
      "Epoch 33/50\n",
      "8s - loss: 0.0490 - acc: 0.9868\n",
      "Epoch 34/50\n",
      "8s - loss: 0.0473 - acc: 0.9874\n",
      "Epoch 35/50\n",
      "8s - loss: 0.0457 - acc: 0.9878\n",
      "Epoch 36/50\n",
      "8s - loss: 0.0440 - acc: 0.9885\n",
      "Epoch 37/50\n",
      "8s - loss: 0.0424 - acc: 0.9891\n",
      "Epoch 38/50\n",
      "8s - loss: 0.0409 - acc: 0.9895\n",
      "Epoch 39/50\n",
      "8s - loss: 0.0393 - acc: 0.9902\n",
      "Epoch 40/50\n",
      "8s - loss: 0.0380 - acc: 0.9899\n",
      "Epoch 41/50\n",
      "10s - loss: 0.0367 - acc: 0.9912\n",
      "Epoch 42/50\n",
      "9s - loss: 0.0353 - acc: 0.9913\n",
      "Epoch 43/50\n",
      "8s - loss: 0.0341 - acc: 0.9914\n",
      "Epoch 44/50\n",
      "9s - loss: 0.0329 - acc: 0.9919\n",
      "Epoch 45/50\n",
      "8s - loss: 0.0319 - acc: 0.9927\n",
      "Epoch 46/50\n",
      "10s - loss: 0.0307 - acc: 0.9928\n",
      "Epoch 47/50\n",
      "9s - loss: 0.0297 - acc: 0.9931\n",
      "Epoch 48/50\n",
      "9s - loss: 0.0288 - acc: 0.9933\n",
      "Epoch 49/50\n",
      "10s - loss: 0.0278 - acc: 0.9937\n",
      "Epoch 50/50\n",
      "9s - loss: 0.0268 - acc: 0.9945\n",
      "10000/10000 [==============================] - 1s     \n",
      "  Loss:  0.0671618557692  , acc: 0.9777\n"
     ]
    }
   ],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(Dense(400, input_dim=784, init='uniform', activation='sigmoid'))\n",
    "model3.add(Dense(10, init='uniform', activation='softmax'))\n",
    "sgd = SGD(lr=0.1) \n",
    "model3.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "model3.fit(mnist.train.images, mnist.train.labels, batch_size=32, nb_epoch=50, verbose=2)\n",
    "scores = model3.evaluate(mnist.test.images, mnist.test.labels);\n",
    "print('  Loss: ', scores[0], ' , acc:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously more nerons in the hidden layer is helpful! The training accuracy is enhanced to 99% though the model on the test sets still has similar performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Add more hidden layers: 3 hidden layers with 200-200-100 neurons\n",
    "#### one hidden layer -> 3 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "19s - loss: 2.3119 - acc: 0.1069\n",
      "Epoch 2/50\n",
      "16s - loss: 2.2996 - acc: 0.1193\n",
      "Epoch 3/50\n",
      "14s - loss: 1.9507 - acc: 0.2768\n",
      "Epoch 4/50\n",
      "16s - loss: 1.3733 - acc: 0.4822\n",
      "Epoch 5/50\n",
      "17s - loss: 0.7101 - acc: 0.7800\n",
      "Epoch 6/50\n",
      "17s - loss: 0.4131 - acc: 0.8841\n",
      "Epoch 7/50\n",
      "18s - loss: 0.2813 - acc: 0.9195\n",
      "Epoch 8/50\n",
      "15s - loss: 0.2248 - acc: 0.9360\n",
      "Epoch 9/50\n",
      "17s - loss: 0.1856 - acc: 0.9458\n",
      "Epoch 10/50\n",
      "16s - loss: 0.1579 - acc: 0.9538\n",
      "Epoch 11/50\n",
      "16s - loss: 0.1360 - acc: 0.9598\n",
      "Epoch 12/50\n",
      "15s - loss: 0.1188 - acc: 0.9653\n",
      "Epoch 13/50\n",
      "14s - loss: 0.1052 - acc: 0.9692\n",
      "Epoch 14/50\n",
      "14s - loss: 0.0937 - acc: 0.9723\n",
      "Epoch 15/50\n",
      "14s - loss: 0.0841 - acc: 0.9747\n",
      "Epoch 16/50\n",
      "14s - loss: 0.0765 - acc: 0.9771\n",
      "Epoch 17/50\n",
      "13s - loss: 0.0677 - acc: 0.9802\n",
      "Epoch 18/50\n",
      "14s - loss: 0.0621 - acc: 0.9816\n",
      "Epoch 19/50\n",
      "20s - loss: 0.0555 - acc: 0.9833\n",
      "Epoch 20/50\n",
      "16s - loss: 0.0510 - acc: 0.9845\n",
      "Epoch 21/50\n",
      "16s - loss: 0.0451 - acc: 0.9869\n",
      "Epoch 22/50\n",
      "15s - loss: 0.0410 - acc: 0.9879\n",
      "Epoch 23/50\n",
      "14s - loss: 0.0377 - acc: 0.9888\n",
      "Epoch 24/50\n",
      "15s - loss: 0.0340 - acc: 0.9897\n",
      "Epoch 25/50\n",
      "14s - loss: 0.0301 - acc: 0.9915\n",
      "Epoch 26/50\n",
      "14s - loss: 0.0256 - acc: 0.9929\n",
      "Epoch 27/50\n",
      "14s - loss: 0.0245 - acc: 0.9936\n",
      "Epoch 28/50\n",
      "14s - loss: 0.0220 - acc: 0.9940\n",
      "Epoch 29/50\n",
      "16s - loss: 0.0198 - acc: 0.9948\n",
      "Epoch 30/50\n",
      "14s - loss: 0.0178 - acc: 0.9955\n",
      "Epoch 31/50\n",
      "14s - loss: 0.0151 - acc: 0.9965\n",
      "Epoch 32/50\n",
      "16s - loss: 0.0136 - acc: 0.9968\n",
      "Epoch 33/50\n",
      "15s - loss: 0.0122 - acc: 0.9969\n",
      "Epoch 34/50\n",
      "16s - loss: 0.0105 - acc: 0.9977\n",
      "Epoch 35/50\n",
      "15s - loss: 0.0083 - acc: 0.9986\n",
      "Epoch 36/50\n",
      "17s - loss: 0.0077 - acc: 0.9986\n",
      "Epoch 37/50\n",
      "14s - loss: 0.0066 - acc: 0.9991\n",
      "Epoch 38/50\n",
      "16s - loss: 0.0060 - acc: 0.9991\n",
      "Epoch 39/50\n",
      "20s - loss: 0.0060 - acc: 0.9989\n",
      "Epoch 40/50\n",
      "16s - loss: 0.0046 - acc: 0.9995\n",
      "Epoch 41/50\n",
      "15s - loss: 0.0040 - acc: 0.9996\n",
      "Epoch 42/50\n",
      "14s - loss: 0.0035 - acc: 0.9998\n",
      "Epoch 43/50\n",
      "14s - loss: 0.0035 - acc: 0.9997\n",
      "Epoch 44/50\n",
      "18s - loss: 0.0031 - acc: 0.9998\n",
      "Epoch 45/50\n",
      "16s - loss: 0.0029 - acc: 0.9999\n",
      "Epoch 46/50\n",
      "15s - loss: 0.0027 - acc: 0.9998\n",
      "Epoch 47/50\n",
      "14s - loss: 0.0026 - acc: 0.9999\n",
      "Epoch 48/50\n",
      "14s - loss: 0.0024 - acc: 0.9999\n",
      "Epoch 49/50\n",
      "16s - loss: 0.0023 - acc: 0.9999\n",
      "Epoch 50/50\n",
      "15s - loss: 0.0023 - acc: 0.9999\n",
      " 9984/10000 [============================>.] - ETA: 0s  Loss:  0.0943529336645  , acc: 0.9784\n"
     ]
    }
   ],
   "source": [
    "model4 = Sequential()\n",
    "model4.add(Dense(200, input_dim=784, init='uniform', activation='sigmoid'))\n",
    "model4.add(Dense(200, init='uniform', activation='sigmoid'))\n",
    "model4.add(Dense(100, init='uniform', activation='sigmoid'))\n",
    "model4.add(Dense(10,  init='uniform', activation='softmax'))\n",
    "sgd = SGD(lr=0.1) \n",
    "model4.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "model4.fit(mnist.train.images, mnist.train.labels, batch_size=16, nb_epoch=50, verbose=2)\n",
    "scores = model4.evaluate(mnist.test.images, mnist.test.labels);\n",
    "print('  Loss: ', scores[0], ' , acc:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With more hidden layers, the trained model shows almost perfect accuracy (99.99%). For the test set, the model also shows to have accuracy of 98%. However, more hidden layers needs more iterations to get converegence. For example, at the beginning few epoches, the model accuracies are 0.1069 and 0.1193, which are worse than that using one hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e) Initialization\n",
    "\n",
    "#### Remove init = 'uniform'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "14s - loss: 1.0521 - acc: 0.6377\n",
      "Epoch 2/50\n",
      "14s - loss: 0.3380 - acc: 0.9008\n",
      "Epoch 3/50\n",
      "14s - loss: 0.2532 - acc: 0.9256\n",
      "Epoch 4/50\n",
      "14s - loss: 0.1984 - acc: 0.9411\n",
      "Epoch 5/50\n",
      "14s - loss: 0.1609 - acc: 0.9523\n",
      "Epoch 6/50\n",
      "14s - loss: 0.1370 - acc: 0.9585\n",
      "Epoch 7/50\n",
      "14s - loss: 0.1172 - acc: 0.9651\n",
      "Epoch 8/50\n",
      "14s - loss: 0.1027 - acc: 0.9688\n",
      "Epoch 9/50\n",
      "14s - loss: 0.0891 - acc: 0.9733\n",
      "Epoch 10/50\n",
      "14s - loss: 0.0800 - acc: 0.9763\n",
      "Epoch 11/50\n",
      "15s - loss: 0.0715 - acc: 0.9777\n",
      "Epoch 12/50\n",
      "15s - loss: 0.0639 - acc: 0.9802\n",
      "Epoch 13/50\n",
      "14s - loss: 0.0560 - acc: 0.9832\n",
      "Epoch 14/50\n",
      "14s - loss: 0.0501 - acc: 0.9850\n",
      "Epoch 15/50\n",
      "15s - loss: 0.0443 - acc: 0.9863\n",
      "Epoch 16/50\n",
      "20s - loss: 0.0392 - acc: 0.9882\n",
      "Epoch 17/50\n",
      "17s - loss: 0.0353 - acc: 0.9897\n",
      "Epoch 18/50\n",
      "17s - loss: 0.0313 - acc: 0.9908\n",
      "Epoch 19/50\n",
      "16s - loss: 0.0275 - acc: 0.9918\n",
      "Epoch 20/50\n",
      "23s - loss: 0.0252 - acc: 0.9922\n",
      "Epoch 21/50\n",
      "21s - loss: 0.0201 - acc: 0.9948\n",
      "Epoch 22/50\n",
      "14s - loss: 0.0188 - acc: 0.9944\n",
      "Epoch 23/50\n",
      "14s - loss: 0.0157 - acc: 0.9958\n",
      "Epoch 24/50\n",
      "15s - loss: 0.0142 - acc: 0.9962\n",
      "Epoch 25/50\n",
      "15s - loss: 0.0120 - acc: 0.9971\n",
      "Epoch 26/50\n",
      "13s - loss: 0.0104 - acc: 0.9975\n",
      "Epoch 27/50\n",
      "14s - loss: 0.0088 - acc: 0.9981\n",
      "Epoch 28/50\n",
      "16s - loss: 0.0076 - acc: 0.9987\n",
      "Epoch 29/50\n",
      "14s - loss: 0.0063 - acc: 0.9991\n",
      "Epoch 30/50\n",
      "14s - loss: 0.0056 - acc: 0.9991\n",
      "Epoch 31/50\n",
      "13s - loss: 0.0047 - acc: 0.9995\n",
      "Epoch 32/50\n",
      "14s - loss: 0.0041 - acc: 0.9997\n",
      "Epoch 33/50\n",
      "14s - loss: 0.0034 - acc: 0.9997\n",
      "Epoch 34/50\n",
      "14s - loss: 0.0031 - acc: 0.9999\n",
      "Epoch 35/50\n",
      "14s - loss: 0.0028 - acc: 0.9998\n",
      "Epoch 36/50\n",
      "14s - loss: 0.0025 - acc: 0.9999\n",
      "Epoch 37/50\n",
      "15s - loss: 0.0023 - acc: 1.0000\n",
      "Epoch 38/50\n",
      "15s - loss: 0.0022 - acc: 0.9999\n",
      "Epoch 39/50\n",
      "13s - loss: 0.0021 - acc: 0.9999\n",
      "Epoch 40/50\n",
      "14s - loss: 0.0019 - acc: 0.9999\n",
      "Epoch 41/50\n",
      "14s - loss: 0.0017 - acc: 0.9999\n",
      "Epoch 42/50\n",
      "14s - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 43/50\n",
      "15s - loss: 0.0015 - acc: 0.9999\n",
      "Epoch 44/50\n",
      "15s - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 45/50\n",
      "17s - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 46/50\n",
      "17s - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 47/50\n",
      "15s - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 48/50\n",
      "17s - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 49/50\n",
      "16s - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 50/50\n",
      "14s - loss: 0.0011 - acc: 1.0000\n",
      "10000/10000 [==============================] - 1s     \n",
      "  Loss:  0.0930763677595  , acc: 0.9801\n"
     ]
    }
   ],
   "source": [
    "model5 = Sequential()\n",
    "model5.add(Dense(200, input_dim=784, activation='sigmoid'))\n",
    "model5.add(Dense(200, activation='sigmoid'))\n",
    "model5.add(Dense(100, activation='sigmoid'))\n",
    "model5.add(Dense(10,  activation='softmax'))\n",
    "sgd = SGD(lr=0.1) \n",
    "model5.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "model5.fit(mnist.train.images, mnist.train.labels, batch_size=16, nb_epoch=50, verbose=2)\n",
    "scores = model5.evaluate(mnist.test.images, mnist.test.labels);\n",
    "print('  Loss: ', scores[0], ' , acc:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous neural network, the weight parameters were initialized by a uniform function. Here we generate the parameters using default initialization function in Keras, showing that the early epoches the model has better performance than one using uniform initialization function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (f) Implement relu activation in the hidden layers\n",
    "\n",
    "#### sigmoid activcation -> relu activaction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "15s - loss: 0.2487 - acc: 0.9227\n",
      "Epoch 2/30\n",
      "17s - loss: 0.1063 - acc: 0.9667\n",
      "Epoch 3/30\n",
      "17s - loss: 0.0752 - acc: 0.9770\n",
      "Epoch 4/30\n",
      "18s - loss: 0.0563 - acc: 0.9823\n",
      "Epoch 5/30\n",
      "19s - loss: 0.0436 - acc: 0.9862\n",
      "Epoch 6/30\n",
      "16s - loss: 0.0368 - acc: 0.9880\n",
      "Epoch 7/30\n",
      "16s - loss: 0.0309 - acc: 0.9903\n",
      "Epoch 8/30\n",
      "16s - loss: 0.0248 - acc: 0.9921\n",
      "Epoch 9/30\n",
      "18s - loss: 0.0187 - acc: 0.9939\n",
      "Epoch 10/30\n",
      "17s - loss: 0.0179 - acc: 0.9943\n",
      "Epoch 11/30\n",
      "14s - loss: 0.0200 - acc: 0.9934\n",
      "Epoch 12/30\n",
      "14s - loss: 0.0131 - acc: 0.9958\n",
      "Epoch 13/30\n",
      "13s - loss: 0.0151 - acc: 0.9951\n",
      "Epoch 14/30\n",
      "14s - loss: 0.0140 - acc: 0.9957\n",
      "Epoch 15/30\n",
      "13s - loss: 0.0104 - acc: 0.9968\n",
      "Epoch 16/30\n",
      "14s - loss: 0.0091 - acc: 0.9973\n",
      "Epoch 17/30\n",
      "14s - loss: 0.0124 - acc: 0.9960\n",
      "Epoch 18/30\n",
      "15s - loss: 0.0078 - acc: 0.9975\n",
      "Epoch 19/30\n",
      "15s - loss: 0.0133 - acc: 0.9957\n",
      "Epoch 20/30\n",
      "14s - loss: 0.0059 - acc: 0.9981\n",
      "Epoch 21/30\n",
      "13s - loss: 0.0048 - acc: 0.9984\n",
      "Epoch 22/30\n",
      "13s - loss: 0.0118 - acc: 0.9961\n",
      "Epoch 23/30\n",
      "13s - loss: 0.0115 - acc: 0.9967\n",
      "Epoch 24/30\n",
      "13s - loss: 0.0053 - acc: 0.9986\n",
      "Epoch 25/30\n",
      "13s - loss: 0.0033 - acc: 0.9989\n",
      "Epoch 26/30\n",
      "13s - loss: 6.9735e-04 - acc: 0.9998\n",
      "Epoch 27/30\n",
      "13s - loss: 0.0012 - acc: 0.9998\n",
      "Epoch 28/30\n",
      "14s - loss: 1.1801e-04 - acc: 1.0000\n",
      "Epoch 29/30\n",
      "18s - loss: 5.0030e-05 - acc: 1.0000\n",
      "Epoch 30/30\n",
      "17s - loss: 3.8850e-05 - acc: 1.0000\n",
      "10000/10000 [==============================] - 2s     \n",
      "  Loss:  0.0860895343051  , acc: 0.9837\n"
     ]
    }
   ],
   "source": [
    "model6 = Sequential()\n",
    "model6.add(Dense(200, input_dim=784, activation='relu'))\n",
    "model6.add(Dense(200, activation='relu'))\n",
    "model6.add(Dense(100, activation='relu'))\n",
    "model6.add(Dense(10,  activation='softmax'))\n",
    "sgd = SGD(lr=0.1) \n",
    "model6.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "model6.fit(mnist.train.images, mnist.train.labels, batch_size=16, nb_epoch=30, verbose=2)\n",
    "scores = model6.evaluate(mnist.test.images, mnist.test.labels);\n",
    "print('  Loss: ', scores[0], ' , acc:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using **relu** even converges faster, since in the previous case using sigmod activation function, the 100% accuracy happened at the 40-epoch, but here at 28-th epoch the model accuracy has been about 100%.\n",
    "\n",
    "#### From the benchmarks, the best network is given by multiple hidden layer. Using relu and default initialization show faster convergence for training the models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prediction\n",
    "\n",
    "**model.predict(X)** gives the probability of each class, and **model.predict_classes(X)** directly gives the class. The highest probability identifies the class which the image is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2.32193374e-21   2.14999036e-14   1.09257340e-14   3.19410599e-17\n",
      "    2.05686366e-14   3.64200596e-21   1.85665959e-21   1.00000000e+00\n",
      "    2.03881241e-16   9.47486989e-13]\n",
      " [  1.03341797e-14   3.32954316e-15   1.00000000e+00   2.59395833e-17\n",
      "    6.71541410e-19   7.84372000e-19   1.95160837e-18   8.02996953e-20\n",
      "    1.56550932e-16   1.12891557e-23]\n",
      " [  1.91516280e-16   1.00000000e+00   2.94681879e-12   2.92172170e-17\n",
      "    2.73391223e-11   5.84677706e-14   3.69494583e-13   3.03999556e-12\n",
      "    4.48173009e-12   6.35018906e-14]\n",
      " [  1.00000000e+00   9.61116974e-19   2.06171094e-10   5.55445116e-15\n",
      "    3.29830585e-15   3.67964021e-14   8.74113004e-11   1.01537196e-15\n",
      "    1.00236307e-17   4.42005721e-15]\n",
      " [  1.47901592e-14   1.17368206e-13   3.26535604e-11   5.74674334e-17\n",
      "    1.00000000e+00   1.10158731e-15   9.50973512e-14   9.46095112e-12\n",
      "    5.46443782e-15   1.31592415e-09]\n",
      " [  5.44018094e-17   1.00000000e+00   1.86798683e-14   8.85652687e-18\n",
      "    1.17272997e-11   1.75686227e-16   5.23730477e-16   1.85655935e-11\n",
      "    3.88071563e-12   1.39642150e-13]\n",
      " [  9.77386659e-16   1.24402114e-16   1.74720828e-12   1.10048631e-16\n",
      "    1.00000000e+00   5.13980622e-13   5.13642514e-13   5.97644018e-14\n",
      "    2.15244955e-09   7.24317273e-10]\n",
      " [  2.47072382e-13   1.05670579e-15   2.03300565e-13   3.60258383e-07\n",
      "    1.30923722e-12   4.00177003e-09   1.21468321e-17   1.84675799e-11\n",
      "    5.92950827e-11   9.99999642e-01]]\n"
     ]
    }
   ],
   "source": [
    "predictions = model5.predict(mnist.test.images)\n",
    "print (predictions[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s     \n",
      "[7 2 1 0 4 1 4 9]\n"
     ]
    }
   ],
   "source": [
    "predictions = model5.predict_classes(mnist.test.images)\n",
    "print (predictions[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeMAAAEkCAYAAAAcmlk0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1361b79e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotExamples(mnist.train.images[:8], mnist.train.labels[:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Revisit the \"mse\" loss function and uniform initialization\n",
    "\n",
    "Here we will revisit the model and study how important to choose a loss function. With **mse** as loss function, even in the same neural network, the convergence is always slower than using **categorical_crossentropy**. The uniform initialization further makes the model worse. However, by choosing **relu**, we can still improve the model performance.\n",
    "\n",
    "### Using \"mse\" loss function converges more slowly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "8s - loss: 0.0903 - acc: 0.1160\n",
      "Epoch 2/50\n",
      "8s - loss: 0.0899 - acc: 0.1168\n",
      "Epoch 3/50\n",
      "10s - loss: 0.0898 - acc: 0.1160\n",
      "Epoch 4/50\n",
      "9s - loss: 0.0898 - acc: 0.1189\n",
      "Epoch 5/50\n",
      "8s - loss: 0.0897 - acc: 0.1362\n",
      "Epoch 6/50\n",
      "7s - loss: 0.0896 - acc: 0.1361\n",
      "Epoch 7/50\n",
      "9s - loss: 0.0895 - acc: 0.1542\n",
      "Epoch 8/50\n",
      "9s - loss: 0.0894 - acc: 0.1612\n",
      "Epoch 9/50\n",
      "11s - loss: 0.0893 - acc: 0.1933\n",
      "Epoch 10/50\n",
      "10s - loss: 0.0891 - acc: 0.1980\n",
      "Epoch 11/50\n",
      "8s - loss: 0.0889 - acc: 0.2237\n",
      "Epoch 12/50\n",
      "9s - loss: 0.0886 - acc: 0.2191\n",
      "Epoch 13/50\n",
      "7s - loss: 0.0882 - acc: 0.2411\n",
      "Epoch 14/50\n",
      "8s - loss: 0.0876 - acc: 0.2248\n",
      "Epoch 15/50\n",
      "8s - loss: 0.0866 - acc: 0.2280\n",
      "Epoch 16/50\n",
      "8s - loss: 0.0846 - acc: 0.2289\n",
      "Epoch 17/50\n",
      "9s - loss: 0.0811 - acc: 0.2535\n",
      "Epoch 18/50\n",
      "8s - loss: 0.0770 - acc: 0.3193\n",
      "Epoch 19/50\n",
      "9s - loss: 0.0734 - acc: 0.3715\n",
      "Epoch 20/50\n",
      "8s - loss: 0.0702 - acc: 0.4072\n",
      "Epoch 21/50\n",
      "8s - loss: 0.0670 - acc: 0.4420\n",
      "Epoch 22/50\n",
      "10s - loss: 0.0640 - acc: 0.4829\n",
      "Epoch 23/50\n",
      "11s - loss: 0.0609 - acc: 0.5285\n",
      "Epoch 24/50\n",
      "9s - loss: 0.0574 - acc: 0.5790\n",
      "Epoch 25/50\n",
      "8s - loss: 0.0535 - acc: 0.6294\n",
      "Epoch 26/50\n",
      "8s - loss: 0.0492 - acc: 0.6748\n",
      "Epoch 27/50\n",
      "7s - loss: 0.0450 - acc: 0.7150\n",
      "Epoch 28/50\n",
      "9s - loss: 0.0412 - acc: 0.7446\n",
      "Epoch 29/50\n",
      "8s - loss: 0.0380 - acc: 0.7675\n",
      "Epoch 30/50\n",
      "7s - loss: 0.0353 - acc: 0.7867\n",
      "Epoch 31/50\n",
      "7s - loss: 0.0329 - acc: 0.8040\n",
      "Epoch 32/50\n",
      "7s - loss: 0.0308 - acc: 0.8167\n",
      "Epoch 33/50\n",
      "7s - loss: 0.0288 - acc: 0.8295\n",
      "Epoch 34/50\n",
      "7s - loss: 0.0271 - acc: 0.8402\n",
      "Epoch 35/50\n",
      "7s - loss: 0.0256 - acc: 0.8486\n",
      "Epoch 36/50\n",
      "8s - loss: 0.0242 - acc: 0.8557\n",
      "Epoch 37/50\n",
      "7s - loss: 0.0230 - acc: 0.8618\n",
      "Epoch 38/50\n",
      "7s - loss: 0.0220 - acc: 0.8671\n",
      "Epoch 39/50\n",
      "9s - loss: 0.0210 - acc: 0.8714\n",
      "Epoch 40/50\n",
      "8s - loss: 0.0203 - acc: 0.8752\n",
      "Epoch 41/50\n",
      "8s - loss: 0.0196 - acc: 0.8791\n",
      "Epoch 42/50\n",
      "9s - loss: 0.0190 - acc: 0.8824\n",
      "Epoch 43/50\n",
      "7s - loss: 0.0185 - acc: 0.8846\n",
      "Epoch 44/50\n",
      "7s - loss: 0.0180 - acc: 0.8870\n",
      "Epoch 45/50\n",
      "8s - loss: 0.0176 - acc: 0.8892\n",
      "Epoch 46/50\n",
      "9s - loss: 0.0173 - acc: 0.8913\n",
      "Epoch 47/50\n",
      "9s - loss: 0.0169 - acc: 0.8929\n",
      "Epoch 48/50\n",
      "8s - loss: 0.0167 - acc: 0.8944\n",
      "Epoch 49/50\n",
      "8s - loss: 0.0164 - acc: 0.8959\n",
      "Epoch 50/50\n",
      "8s - loss: 0.0161 - acc: 0.8973\n",
      " 9920/10000 [============================>.] - ETA: 0s  Loss:  0.0156185688918  , acc: 0.8984\n"
     ]
    }
   ],
   "source": [
    "model7 = Sequential()\n",
    "model7.add(Dense(200, input_dim=784, activation='sigmoid'))\n",
    "model7.add(Dense(200, activation='sigmoid'))\n",
    "model7.add(Dense(100, activation='sigmoid'))\n",
    "model7.add(Dense(10, activation='softmax'))\n",
    "sgd = SGD(lr=0.1) \n",
    "model7.compile(loss='mse', optimizer=sgd, metrics=['accuracy'])\n",
    "model7.fit(mnist.train.images, mnist.train.labels, batch_size=32, nb_epoch=50, verbose=2)\n",
    "scores = model7.evaluate(mnist.test.images, mnist.test.labels);\n",
    "print('  Loss: ', scores[0], ' , acc:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using uniform initialization + mse loss shows poor performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "8s - loss: 0.0900 - acc: 0.1095\n",
      "Epoch 2/30\n",
      "7s - loss: 0.0900 - acc: 0.1115\n",
      "Epoch 3/30\n",
      "7s - loss: 0.0900 - acc: 0.1117\n",
      "Epoch 4/30\n",
      "7s - loss: 0.0900 - acc: 0.1114\n",
      "Epoch 5/30\n",
      "8s - loss: 0.0900 - acc: 0.1117\n",
      "Epoch 6/30\n",
      "10s - loss: 0.0900 - acc: 0.1117\n",
      "Epoch 7/30\n",
      "8s - loss: 0.0900 - acc: 0.1119\n",
      "Epoch 8/30\n",
      "9s - loss: 0.0900 - acc: 0.1121\n",
      "Epoch 9/30\n",
      "10s - loss: 0.0900 - acc: 0.1117\n",
      "Epoch 10/30\n",
      "7s - loss: 0.0900 - acc: 0.1123\n",
      "Epoch 11/30\n",
      "7s - loss: 0.0900 - acc: 0.1113\n",
      "Epoch 12/30\n",
      "10s - loss: 0.0900 - acc: 0.1120\n",
      "Epoch 13/30\n",
      "10s - loss: 0.0900 - acc: 0.1120\n",
      "Epoch 14/30\n",
      "10s - loss: 0.0900 - acc: 0.1123\n",
      "Epoch 15/30\n",
      "9s - loss: 0.0900 - acc: 0.1121\n",
      "Epoch 16/30\n",
      "7s - loss: 0.0900 - acc: 0.1123\n",
      "Epoch 17/30\n",
      "8s - loss: 0.0900 - acc: 0.1120\n",
      "Epoch 18/30\n",
      "8s - loss: 0.0900 - acc: 0.1117\n",
      "Epoch 19/30\n",
      "9s - loss: 0.0900 - acc: 0.1119\n",
      "Epoch 20/30\n",
      "8s - loss: 0.0900 - acc: 0.1122\n",
      "Epoch 21/30\n",
      "7s - loss: 0.0900 - acc: 0.1123\n",
      "Epoch 22/30\n",
      "11s - loss: 0.0900 - acc: 0.1123\n",
      "Epoch 23/30\n",
      "14s - loss: 0.0900 - acc: 0.1123\n",
      "Epoch 24/30\n",
      "12s - loss: 0.0900 - acc: 0.1123\n",
      "Epoch 25/30\n",
      "9s - loss: 0.0900 - acc: 0.1123\n",
      "Epoch 26/30\n",
      "8s - loss: 0.0900 - acc: 0.1123\n",
      "Epoch 27/30\n",
      "7s - loss: 0.0900 - acc: 0.1120\n",
      "Epoch 28/30\n",
      "7s - loss: 0.0900 - acc: 0.1119\n",
      "Epoch 29/30\n",
      "7s - loss: 0.0900 - acc: 0.1120\n",
      "Epoch 30/30\n",
      "7s - loss: 0.0900 - acc: 0.1119\n",
      " 9888/10000 [============================>.] - ETA: 0s  Loss:  0.0899695995569  , acc: 0.1135\n"
     ]
    }
   ],
   "source": [
    "model8 = Sequential()\n",
    "model8.add(Dense(200, input_dim=784, init='uniform', activation='sigmoid'))\n",
    "model8.add(Dense(200, init='uniform', activation='sigmoid'))\n",
    "model8.add(Dense(100, init='uniform', activation='sigmoid'))\n",
    "model8.add(Dense(10, init='uniform', activation='softmax'))\n",
    "sgd = SGD(lr=0.1) \n",
    "model8.compile(loss='mse', optimizer=sgd, metrics=['accuracy'])\n",
    "model8.fit(mnist.train.images, mnist.train.labels, batch_size=32, nb_epoch=30, verbose=2)\n",
    "scores = model8.evaluate(mnist.test.images, mnist.test.labels);\n",
    "print('  Loss: ', scores[0], ' , acc:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using relu activaction function slightly better than using sigmoid\n",
    "\n",
    "In the previous case, we saw the trained model got stuck to have low accuracy. However, using **relu** is helpful.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "8s - loss: 0.0900 - acc: 0.1209\n",
      "Epoch 2/50\n",
      "8s - loss: 0.0899 - acc: 0.1173\n",
      "Epoch 3/50\n",
      "9s - loss: 0.0897 - acc: 0.1925\n",
      "Epoch 4/50\n",
      "9s - loss: 0.0892 - acc: 0.2190\n",
      "Epoch 5/50\n",
      "7s - loss: 0.0848 - acc: 0.2252\n",
      "Epoch 6/50\n",
      "7s - loss: 0.0761 - acc: 0.3729\n",
      "Epoch 7/50\n",
      "7s - loss: 0.0491 - acc: 0.6571\n",
      "Epoch 8/50\n",
      "7s - loss: 0.0268 - acc: 0.8260\n",
      "Epoch 9/50\n",
      "7s - loss: 0.0208 - acc: 0.8652\n",
      "Epoch 10/50\n",
      "7s - loss: 0.0181 - acc: 0.8823\n",
      "Epoch 11/50\n",
      "7s - loss: 0.0164 - acc: 0.8934\n",
      "Epoch 12/50\n",
      "8s - loss: 0.0150 - acc: 0.9032\n",
      "Epoch 13/50\n",
      "7s - loss: 0.0137 - acc: 0.9114\n",
      "Epoch 14/50\n",
      "10s - loss: 0.0127 - acc: 0.9188\n",
      "Epoch 15/50\n",
      "8s - loss: 0.0116 - acc: 0.9256\n",
      "Epoch 16/50\n",
      "8s - loss: 0.0107 - acc: 0.9314\n",
      "Epoch 17/50\n",
      "8s - loss: 0.0099 - acc: 0.9379\n",
      "Epoch 18/50\n",
      "7s - loss: 0.0092 - acc: 0.9415\n",
      "Epoch 19/50\n",
      "7s - loss: 0.0086 - acc: 0.9461\n",
      "Epoch 20/50\n",
      "7s - loss: 0.0080 - acc: 0.9495\n",
      "Epoch 21/50\n",
      "7s - loss: 0.0075 - acc: 0.9527\n",
      "Epoch 22/50\n",
      "7s - loss: 0.0071 - acc: 0.9553\n",
      "Epoch 23/50\n",
      "8s - loss: 0.0066 - acc: 0.9589\n",
      "Epoch 24/50\n",
      "7s - loss: 0.0063 - acc: 0.9619\n",
      "Epoch 25/50\n",
      "7s - loss: 0.0059 - acc: 0.9639\n",
      "Epoch 26/50\n",
      "7s - loss: 0.0056 - acc: 0.9663\n",
      "Epoch 27/50\n",
      "8s - loss: 0.0053 - acc: 0.9678\n",
      "Epoch 28/50\n",
      "8s - loss: 0.0051 - acc: 0.9693\n",
      "Epoch 29/50\n",
      "8s - loss: 0.0049 - acc: 0.9706\n",
      "Epoch 30/50\n",
      "7s - loss: 0.0046 - acc: 0.9726\n",
      "Epoch 31/50\n",
      "8s - loss: 0.0044 - acc: 0.9735\n",
      "Epoch 32/50\n",
      "8s - loss: 0.0042 - acc: 0.9748\n",
      "Epoch 33/50\n",
      "7s - loss: 0.0040 - acc: 0.9764\n",
      "Epoch 34/50\n",
      "7s - loss: 0.0039 - acc: 0.9775\n",
      "Epoch 35/50\n",
      "7s - loss: 0.0037 - acc: 0.9786\n",
      "Epoch 36/50\n",
      "7s - loss: 0.0036 - acc: 0.9789\n",
      "Epoch 37/50\n",
      "7s - loss: 0.0034 - acc: 0.9803\n",
      "Epoch 38/50\n",
      "7s - loss: 0.0033 - acc: 0.9817\n",
      "Epoch 39/50\n",
      "7s - loss: 0.0032 - acc: 0.9814\n",
      "Epoch 40/50\n",
      "7s - loss: 0.0031 - acc: 0.9828\n",
      "Epoch 41/50\n",
      "7s - loss: 0.0030 - acc: 0.9835\n",
      "Epoch 42/50\n",
      "7s - loss: 0.0029 - acc: 0.9840\n",
      "Epoch 43/50\n",
      "7s - loss: 0.0027 - acc: 0.9850\n",
      "Epoch 44/50\n",
      "7s - loss: 0.0027 - acc: 0.9854\n",
      "Epoch 45/50\n",
      "7s - loss: 0.0026 - acc: 0.9861\n",
      "Epoch 46/50\n",
      "7s - loss: 0.0025 - acc: 0.9866\n",
      "Epoch 47/50\n",
      "7s - loss: 0.0024 - acc: 0.9873\n",
      "Epoch 48/50\n",
      "7s - loss: 0.0023 - acc: 0.9875\n",
      "Epoch 49/50\n",
      "7s - loss: 0.0022 - acc: 0.9882\n",
      "Epoch 50/50\n",
      "7s - loss: 0.0022 - acc: 0.9891\n",
      " 9888/10000 [============================>.] - ETA: 0s  Loss:  0.00430187083261  , acc: 0.9728\n"
     ]
    }
   ],
   "source": [
    "model9 = Sequential()\n",
    "model9.add(Dense(200, input_dim=784, init='uniform', activation='relu'))\n",
    "model9.add(Dense(200, init='uniform', activation='relu'))\n",
    "model9.add(Dense(100, init='uniform', activation='relu'))\n",
    "model9.add(Dense(10, init='uniform', activation='softmax'))\n",
    "sgd = SGD(lr=0.1) \n",
    "model9.compile(loss='mse', optimizer=sgd, metrics=['accuracy'])\n",
    "model9.fit(mnist.train.images, mnist.train.labels, batch_size=32, nb_epoch=50, verbose=2)\n",
    "scores = model9.evaluate(mnist.test.images, mnist.test.labels);\n",
    "print('  Loss: ', scores[0], ' , acc:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The best neural network is given by the 3-hidden-layer network structure 784-200-200-100-10, and using **relu** as activation function in the hidden layers and **softmax** in the output layer as well as the default initialization to train the models. For the multilcass classification, **categorical_crossentropy** loss function is preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
